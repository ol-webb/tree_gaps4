{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gap_folders_for_filters(base_folder, df, filterp=False, scalep=False,np=1):\n",
    "    \"\"\"\n",
    "    Processes gap folders and adds time series data to given dataframes.\n",
    "\n",
    "        base_folder (str): The base folder containing gap folders.\n",
    "        df (pd.DataFrame): The dataframe with gap information.\n",
    "        series_dataframe (pd.DataFrame): Dataframe to add gap time series data to.\n",
    "        random_series_dataframe (pd.DataFrame): Dataframe to add random time series data to.\n",
    "        np: number of random points, default is 3\n",
    "    \"\"\"\n",
    "    # Create a list of all folders/filepaths for all gaps\n",
    "    gap_folders = [os.path.join(base_folder, folder) for folder in os.listdir(base_folder) if folder.startswith('gap')]\n",
    "\n",
    "    start_date = datetime(2017, 11, 11)\n",
    "    end_date = datetime.now()\n",
    "    date_range = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "    dataframe_list=[]\n",
    "    ## iterate over number of points, create a dataframe for each point\n",
    "    for point in range(np):\n",
    "\n",
    "        ## initialize dataframe by previously defined date range\n",
    "        random_series_dataframe = pd.DataFrame(index=date_range)\n",
    "\n",
    "\n",
    "        # Iterate over each gap folder and process the data\n",
    "        for gap_folder in gap_folders:\n",
    "\n",
    "            gap_number = int(gap_folder.split('gap')[-1])\n",
    "            center_coord = read_gap_coordinates(gap_number, df)\n",
    "            random_series_dataframe = add_random_time_series_to_df(random_series_dataframe, gap_folder, center_coord, num_points=1, radius=200, attribute_table=df, filterp=filterp, scalep=scalep)\n",
    "\n",
    "        dataframe_list.append(random_series_dataframe)\n",
    "\n",
    "    return dataframe_list\n",
    "\n",
    "\n",
    "def lee_filter(data):\n",
    "    \"\"\"Apply Lee filter to a 2D array with time series in rows and 9 columns for neighbors.\"\"\"\n",
    "    filtered_data = np.zeros(data.shape[0])\n",
    "    pad_size = 1\n",
    "    padded_data = np.pad(data, ((pad_size, pad_size), (0, 0)), mode='edge')\n",
    "    \n",
    "    noise_variance = np.var(data)  # Estimate noise variance from the entire data\n",
    "\n",
    "    for t in range(data.shape[0]):\n",
    "        local_window = padded_data[t:t + 3, :]\n",
    "        local_mean = np.mean(local_window, axis=0)\n",
    "        local_variance = np.var(local_window, axis=0)\n",
    "        k = local_variance / (local_variance + noise_variance)\n",
    "        filtered_data[t] = local_mean[0] + k[0] * (data[t, 0] - local_mean[0])\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Plotting Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "\n",
    "# Image Processing\n",
    "import rasterio\n",
    "from skimage.restoration import denoise_nl_means, estimate_sigma\n",
    "from skimage import img_as_float\n",
    "\n",
    "# Signal Processing and Statistics\n",
    "from scipy.signal import find_peaks, savgol_filter\n",
    "from scipy.stats import skew, kurtosis, entropy as scipy_entropy\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.ndimage import median_filter\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "# Time Series Analysis and Filtering\n",
    "from pykalman import KalmanFilter\n",
    "import nolds\n",
    "from scipy.signal import wiener\n",
    "import bm3d\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import uniform_filter\n",
    "\n",
    "\n",
    "# Coordinate Transformation\n",
    "from pyproj import Transformer\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated PATH:\n",
      "c:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python311;c:\\Users\\olive\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\dotnet\\;C:\\Program Files\\Pandoc\\;C:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python311;C:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python311\\Scripts;C:\\Users\\olive\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64;C:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python311\\Scripts;C:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python311;C:\\Users\\olive\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\olive\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Program Files\\esa-snap\\bin;C:\\Users\\olive\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64\\;C:\\Program Files\\Common Files\\Oracle\\Java\\javapath;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\java8path;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\dotnet\\;C:\\Program Files\\Pandoc\\;C:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python311;C:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python311\\Scripts;C:\\Users\\olive\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64;C:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python311\\Scripts;C:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python311;C:\\Users\\olive\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\olive\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Program Files\\esa-snap\\bin;C:\\Users\\olive\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64\\;C:\\Users\\olive\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64\n",
      "LaTeX configuration:\n",
      "MiKTeX-pdfTeX 4.19 (MiKTeX 24.4)\n",
      "Â© 1982 D. E. Knuth, Â© 1996-2023 HÃ n Tháº¿ ThÃ nh\n",
      "TeX is a trademark of the American Mathematical Society.\n",
      "using bzip2 version 1.0.8, 13-Jul-2019\n",
      "compiled with curl version 8.4.0; using libcurl/8.4.0 Schannel\n",
      "compiled with expat version 2.5; using expat_2.5.0\n",
      "compiled with jpeg version 9.5\n",
      "compiled with liblzma version 50040002; using 50040002\n",
      "compiled with libpng version 1.6.39; using 1.6.39\n",
      "compiled with libressl version LibreSSL 3.8.1; using LibreSSL 3.8.1\n",
      "compiled with MiKTeX Application Framework version 4.8; using 4.8\n",
      "compiled with MiKTeX Core version 4.24; using 4.24\n",
      "compiled with MiKTeX Archive Extractor version 4.1; using 4.1\n",
      "compiled with MiKTeX Package Manager version 4.10; using 4.10\n",
      "compiled with uriparser version 0.9.7\n",
      "compiled with xpdf version 4.04\n",
      "compiled with zlib version 1.2.13; using 1.2.13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add MiKTeX bin directory to PATH\n",
    "os.environ['PATH'] += os.pathsep + r'C:\\Users\\olive\\AppData\\Local\\Programs\\MiKTeX\\miktex\\bin\\x64'\n",
    "\n",
    "# Verify the update\n",
    "print(\"Updated PATH:\")\n",
    "print(os.environ['PATH'])\n",
    "\n",
    "# Print LaTeX configuration\n",
    "print(\"LaTeX configuration:\")\n",
    "print(os.popen(\"latex --version\").read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set specific font sizes\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,          # Global font size\n",
    "    'axes.titlesize': 11,     # Title font size\n",
    "    'axes.labelsize': 11,     # X and Y axis labels font size\n",
    "    'xtick.labelsize': 9,    # X-axis tick labels font size\n",
    "    'ytick.labelsize': 9,    # Y-axis tick labels font size\n",
    "    'legend.fontsize': 10,    # Legend font size\n",
    "})\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('saved_attribute_table.csv')\n",
    "attribute_table2=pd.read_csv('attribute_table_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'fid' column is the index in both dataframes\n",
    "df.set_index('fid', inplace=True)\n",
    "attribute_table2.set_index('fid', inplace=True)\n",
    "\n",
    "# Update the centroid_x and centroid_y columns in df with the values from attribute_table2\n",
    "df.update(attribute_table2[['centroid_x', 'centroid_y']])\n",
    "\n",
    "# Reset the index to return 'fid' as a column\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_forest_mask(tif_path, utm_x, utm_y, proj):\n",
    "    \"\"\"\n",
    "    Returns forest coverage for a given point\n",
    "    \n",
    "    Parameters:\n",
    "    tif_path (str): The path to the forest coverage file.\n",
    "    utm_x (float): The UTM easting coordinate.\n",
    "    utm_y (float): The UTM northing coordinate.\n",
    "    \n",
    "    Returns:\n",
    "    int: Forest coverage percentage\n",
    "    \"\"\"\n",
    "\n",
    "    with rasterio.open(tif_path) as dataset:\n",
    "        # Initialize the transformer\n",
    "        transformer_to_raster_crs = Transformer.from_crs(proj, dataset.crs, always_xy=True)\n",
    "        \n",
    "        # Convert UTM coordinates to the raster's CRS\n",
    "        raster_x, raster_y = transformer_to_raster_crs.transform(utm_x, utm_y)\n",
    "        \n",
    "        # Calculate the pixel coordinates\n",
    "        pixel_col, pixel_row = ~dataset.transform * (raster_x, raster_y)\n",
    "        pixel_col = int(pixel_col)\n",
    "        pixel_row = int(pixel_row)\n",
    "\n",
    "        \n",
    "        # Check if the pixel coordinates are within the raster's dimensions\n",
    "        if 0 <= pixel_col < dataset.width and 0 <= pixel_row < dataset.height:\n",
    "            # Read the value at the pixel coordinates\n",
    "            data = dataset.read(1, window=((pixel_row, pixel_row + 1), (pixel_col, pixel_col + 1)))\n",
    "            pixel_value = data[0, 0]\n",
    "            return pixel_value\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "def generate_random_coordinates(center, radius, num_points=10):\n",
    "    \"\"\"\n",
    "    Generate random coordinates within a given radius from a center point.\n",
    "    \n",
    "    Parameters:\n",
    "    center (tuple): A tuple containing the UTM coordinates of the center point (easting, northing).\n",
    "    radius (float): The radius within which to generate the random points (in meters).\n",
    "    num_points (int): The number of random points to generate. Default is 10.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of tuples containing the UTM coordinates of the random points.\n",
    "    \"\"\"\n",
    "    center_easting, center_northing = center\n",
    "    points = []\n",
    "    valid_points = 0\n",
    "\n",
    "    if str(center_easting).startswith('21'):\n",
    "        tif_path = '00N_060W.tif'\n",
    "        proj='EPSG:29181'\n",
    "    else:\n",
    "        tif_path = '00N_070W.tif'\n",
    "        proj='EPSG:32720'\n",
    "\n",
    "\n",
    "    while valid_points < num_points:\n",
    "        angle = random.uniform(0, 2 * math.pi)\n",
    "        distance = random.uniform(0, radius)\n",
    "        \n",
    "        offset_easting = distance * math.cos(angle)\n",
    "        offset_northing = distance * math.sin(angle)\n",
    "        \n",
    "        new_easting = center_easting + offset_easting\n",
    "        new_northing = center_northing + offset_northing\n",
    "        \n",
    "        pixel_value = check_forest_mask(tif_path, new_easting, new_northing,proj)\n",
    "        \n",
    "        if pixel_value == 100:  ## verify its forest\n",
    "            points.append((new_easting, new_northing))\n",
    "            valid_points += 1\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_tif_files(folder_path):\n",
    "    return sorted(glob.glob(os.path.join(folder_path, '*.tif')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utm_to_pixel(dataset, utm_x, utm_y):\n",
    "    transform = dataset.transform\n",
    "    inv_transform = ~transform\n",
    "    pixel_x, pixel_y = inv_transform * (utm_x, utm_y)\n",
    "    return int(pixel_x), int(pixel_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_value(tif_file, utm_x, utm_y, apply_filter=False):\n",
    "    with rasterio.open(tif_file) as dataset:\n",
    "        # Read the image data\n",
    "        band1 = dataset.read(1)\n",
    "        \n",
    "        if apply_filter:\n",
    "            # Convert to float for the filter\n",
    "            band1_float = img_as_float(band1)\n",
    "            # Estimate the noise standard deviation from the data\n",
    "            sigma_est = np.mean(estimate_sigma(band1_float))\n",
    "            # Apply the Non-Local Means (NLM) filter\n",
    "            denoised_band1 = denoise_nl_means(band1_float, h=1.15 * sigma_est, fast_mode=True,\n",
    "                                              patch_size=5, patch_distance=6)\n",
    "            # Use the denoised image\n",
    "            image_data = denoised_band1\n",
    "        else:\n",
    "            # Use the original image\n",
    "            image_data = band1\n",
    "\n",
    "        # Convert UTM point to pixel coordinates\n",
    "        pixel_x, pixel_y = utm_to_pixel(dataset, utm_x, utm_y)\n",
    "        pixel_value = image_data[pixel_y, pixel_x]\n",
    "        return pixel_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates for gap82: (217941.875567938, 9551027.85076804)\n"
     ]
    }
   ],
   "source": [
    "def read_gap_coordinates(gap_number, df):\n",
    "    \"\"\"\n",
    "    Given a gap number, return the centroid coordinates from the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    gap_number (int): The gap number (1-based index).\n",
    "    df (pd.DataFrame): The DataFrame containing the coordinates.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: A tuple containing the (x, y) coordinates.\n",
    "    \"\"\"\n",
    "    # Convert 1-based index to 0-based index\n",
    "    row_index = gap_number - 1\n",
    "    \n",
    "    # Get the coordinates from the DataFrame\n",
    "    x_coord = df.at[row_index, 'centroid_x']\n",
    "    y_coord = df.at[row_index, 'centroid_y']\n",
    "    \n",
    "    return (x_coord, y_coord)\n",
    "\n",
    "# Example usage\n",
    "gap_number = 82\n",
    "coordinates = read_gap_coordinates(gap_number, df)\n",
    "print(f\"Coordinates for gap{gap_number}: {coordinates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_series_for_coordinate(folder_path, coord, filter='False', scale=False):\n",
    "    tif_files = list_tif_files(folder_path)\n",
    "    dates = []\n",
    "    coord_values = []\n",
    "\n",
    "    # Loop through each .tif file\n",
    "    for tif_file in tif_files:\n",
    "        # Extract the date from the filename\n",
    "        date_str = os.path.basename(tif_file).split('-')[-3] + '-' + os.path.basename(tif_file).split('-')[-2] + '-' + os.path.basename(tif_file).split('-')[-1].split('.')[0]\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        dates.append(date)\n",
    "\n",
    "        # Get pixel value for the specified coordinate\n",
    "        coord_value = get_pixel_value(tif_file, coord[0], coord[1], filter != 'False')\n",
    "        coord_values.append(coord_value)\n",
    "\n",
    "    if scale:\n",
    "        coord_values = [10**(value / 10) for value in coord_values]\n",
    "\n",
    "    # Apply the chosen filter\n",
    "    if filter == 'kalman':\n",
    "        kf = KalmanFilter(initial_state_mean=coord_values[0], n_dim_obs=1)\n",
    "        coord_values, _ = kf.smooth(coord_values)  # Extract the smoothed state estimates\n",
    "        coord_values = coord_values.flatten()\n",
    "    elif filter == 'savgol':\n",
    "        coord_values = savgol_filter(coord_values, window_length=5, polyorder=2)\n",
    "\n",
    "    return dates, coord_values\n",
    "\n",
    "\n",
    "def extract_time_series_for_random_points(folder_path, center_coord, radius, num_points, filter='False', scale=False):\n",
    "    tif_files = list_tif_files(folder_path)\n",
    "    dates = []\n",
    "    random_points = generate_random_coordinates(center_coord, radius, num_points)\n",
    "    random_points_time_series = [[] for _ in range(num_points)]\n",
    "\n",
    "    for tif_file in tif_files:\n",
    "        date_str = os.path.basename(tif_file).split('-')[-3] + '-' + os.path.basename(tif_file).split('-')[-2] + '-' + os.path.basename(tif_file).split('-')[-1].split('.')[0]\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        dates.append(date)\n",
    "\n",
    "        for i, (utm_x, utm_y) in enumerate(random_points):\n",
    "            coord_value = get_pixel_value(tif_file, utm_x, utm_y, filter != 'False')\n",
    "            random_points_time_series[i].append(coord_value)\n",
    "\n",
    "    if scale:\n",
    "        random_points_time_series = [[10**(value / 10) for value in series] for series in random_points_time_series]\n",
    "\n",
    "    # Apply the chosen filter\n",
    "    if filter == 'kalman':\n",
    "        for i in range(num_points):\n",
    "            kf = KalmanFilter(initial_state_mean=random_points_time_series[i][0], n_dim_obs=1)\n",
    "            random_points_time_series[i], _ = kf.smooth(random_points_time_series[i])  # Extract the smoothed state estimates\n",
    "            random_points_time_series[i] = random_points_time_series[i].flatten()\n",
    "    elif filter == 'savgol':\n",
    "        random_points_time_series = [savgol_filter(series, window_length=5, polyorder=2) for series in random_points_time_series]\n",
    "\n",
    "    return dates, random_points_time_series\n",
    "\n",
    "\n",
    "def get_pixel_value(tif_file, utm_x, utm_y, apply_filter=False):\n",
    "    with rasterio.open(tif_file) as dataset:\n",
    "        # Read the image data\n",
    "        band1 = dataset.read(1)\n",
    "        \n",
    "        if apply_filter:\n",
    "            # Convert to float for the filter\n",
    "            band1_float = img_as_float(band1)\n",
    "            # Estimate the noise standard deviation from the data\n",
    "            sigma_est = np.mean(estimate_sigma(band1_float))\n",
    "            # Apply the Non-Local Means (NLM) filter\n",
    "            denoised_band1 = denoise_nl_means(band1_float, h=1.15 * sigma_est, fast_mode=True,\n",
    "                                              patch_size=5, patch_distance=6)\n",
    "            # Use the denoised image\n",
    "            image_data = denoised_band1\n",
    "        else:\n",
    "            # Use the original image\n",
    "            image_data = band1\n",
    "\n",
    "        # Convert UTM point to pixel coordinates\n",
    "        pixel_x, pixel_y = utm_to_pixel(dataset, utm_x, utm_y)\n",
    "        pixel_value = image_data[pixel_y, pixel_x]\n",
    "        return pixel_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'downloaded_files/gap83'\n",
    "coord = read_gap_coordinates(83,df)\n",
    "dates, coord_values = extract_time_series_for_random_points(folder_path, coord,100,10, filter=False, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_series_to_df(series_dataframe_, gap_folder, attribute_table,filterp=False,scalep=False):\n",
    "    # Extract gap number from the folder name\n",
    "    gap_number = int(gap_folder.split('gap')[-1])\n",
    "    \n",
    "    # Get the coordinates for the current gap folder\n",
    "    gap_coord = read_gap_coordinates(gap_number, attribute_table)\n",
    "    \n",
    "    # Extract the time series data for the current gap folder\n",
    "    dates, coord_values = extract_time_series_for_coordinate(gap_folder, gap_coord, filter=filterp, scale=scalep)\n",
    "    \n",
    "    # Create a DataFrame for the current gap folder\n",
    "    gap_df = pd.DataFrame({'Date': dates, f'gap{gap_number}': coord_values})\n",
    "    \n",
    "    # Set the 'Date' column as the index\n",
    "    gap_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Merge with the main DataFrame, aligning on the 'Date' index\n",
    "    series_dataframe_ = series_dataframe_.merge(gap_df, left_index=True, right_index=True, how='outer')\n",
    "    \n",
    "    return series_dataframe_\n",
    "\n",
    "\n",
    "def add_random_time_series_to_df(series_dataframe_, gap_folder, center_coord, num_points=3, radius=100, attribute_table=None,filterp=False,scalep=False):\n",
    "    # Extract gap number from the folder name\n",
    "    gap_number = int(gap_folder.split('gap')[-1])\n",
    "\n",
    "    # Extract the time series data for random points in the current gap folder\n",
    "    dates, random_points_time_series = extract_time_series_for_random_points(gap_folder, center_coord, radius, num_points, filter=filterp, scale=scalep)\n",
    "\n",
    "    # Create a DataFrame for the random points time series\n",
    "    for i in range(num_points):\n",
    "        gap_df = pd.DataFrame({'Date': dates, f'gap{gap_number}_random{i+1}': random_points_time_series[i]})\n",
    "        gap_df.set_index('Date', inplace=True)\n",
    "        series_dataframe_ = series_dataframe_.merge(gap_df, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "    return series_dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gap_folders(base_folder, df, series_dataframe, random_series_dataframe, filterp=False, scalep=False,np=3):\n",
    "    \"\"\"\n",
    "    Processes gap folders and adds time series data to given dataframes.\n",
    "\n",
    "        base_folder (str): The base folder containing gap folders.\n",
    "        df (pd.DataFrame): The dataframe with gap information.\n",
    "        series_dataframe (pd.DataFrame): Dataframe to add gap time series data to.\n",
    "        random_series_dataframe (pd.DataFrame): Dataframe to add random time series data to.\n",
    "        np: number of random points, default is 3\n",
    "    \"\"\"\n",
    "    # Create a list of all folders/filepaths for all gaps\n",
    "    gap_folders = [os.path.join(base_folder, folder) for folder in os.listdir(base_folder) if folder.startswith('gap')]\n",
    "\n",
    "    # Iterate over each gap folder and process the data\n",
    "    for gap_folder in gap_folders:\n",
    "        series_dataframe = add_time_series_to_df(series_dataframe, gap_folder, df, filterp=filterp, scalep=scalep)\n",
    "\n",
    "        gap_number = int(gap_folder.split('gap')[-1])\n",
    "        center_coord = read_gap_coordinates(gap_number, df)\n",
    "        random_series_dataframe = add_random_time_series_to_df(random_series_dataframe, gap_folder, center_coord, num_points=np, radius=200, attribute_table=df, filterp=filterp, scalep=scalep)\n",
    "\n",
    "    return series_dataframe, random_series_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract time series for neighbours functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2017, 11, 11)\n",
    "end_date = datetime.now()\n",
    "date_range = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "df_gen1 = pd.DataFrame(index=date_range)\n",
    "df_gen2 = pd.DataFrame(index=date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert UTM coordinates to pixel coordinates with an offset\n",
    "def unique_utm_to_pixel(dataset, utm_x, utm_y, offset_x=0, offset_y=0):\n",
    "    transform = dataset.transform\n",
    "    inv_transform = ~transform\n",
    "    pixel_x, pixel_y = inv_transform * (utm_x, utm_y)\n",
    "    return int(pixel_x) + offset_x, int(pixel_y) + offset_y\n",
    "\n",
    "\n",
    "# Function to get pixel value from a TIF file for a given UTM coordinate\n",
    "def unique_get_pixel_value(tif_file, utm_x, utm_y, offset_x=0, offset_y=0, apply_filter=False):\n",
    "    with rasterio.open(tif_file) as dataset:\n",
    "        band1 = dataset.read(1)\n",
    "        \n",
    "        if apply_filter:\n",
    "            band1_float = img_as_float(band1)\n",
    "            sigma_est = np.mean(estimate_sigma(band1_float))\n",
    "            denoised_band1 = denoise_nl_means(band1_float, h=1.15 * sigma_est, fast_mode=True,\n",
    "                                              patch_size=5, patch_distance=6)\n",
    "            image_data = denoised_band1\n",
    "        else:\n",
    "            image_data = band1\n",
    "\n",
    "        pixel_x, pixel_y = unique_utm_to_pixel(dataset, utm_x, utm_y, offset_x, offset_y)\n",
    "        pixel_value = image_data[pixel_y, pixel_x]\n",
    "        return pixel_value\n",
    "\n",
    "\n",
    "# Function to extract time series for a given coordinate\n",
    "def unique_extract_time_series_for_coordinate(folder_path, coord, offset_x=0, offset_y=0, filter='False', scale=False):\n",
    "    tif_files = list_tif_files(folder_path)\n",
    "    dates = []\n",
    "    coord_values = []\n",
    "\n",
    "    for tif_file in tif_files:\n",
    "        date_str = os.path.basename(tif_file).split('-')[-3] + '-' + os.path.basename(tif_file).split('-')[-2] + '-' + os.path.basename(tif_file).split('-')[-1].split('.')[0]\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        dates.append(date)\n",
    "\n",
    "        coord_value = unique_get_pixel_value(tif_file, coord[0], coord[1], offset_x, offset_y, apply_filter=(filter != 'False'))\n",
    "        coord_values.append(coord_value)\n",
    "\n",
    "    if scale:\n",
    "        coord_values = [10**(value / 10) for value in coord_values]\n",
    "\n",
    "    if filter == 'kalman':\n",
    "        kf = KalmanFilter(initial_state_mean=coord_values[0], n_dim_obs=1)\n",
    "        coord_values, _ = kf.smooth(coord_values)\n",
    "        coord_values = coord_values.flatten()\n",
    "    elif filter == 'savgol':\n",
    "        coord_values = savgol_filter(coord_values, window_length=5, polyorder=2)\n",
    "\n",
    "    return dates, coord_values\n",
    "\n",
    "# Function to extract time series for surrounding pixels\n",
    "def unique_extract_time_series_for_surrounding_pixels(folder_path, central_coord, filter='False', scale=False):\n",
    "    offsets = [(0, 0), (0, 1), (0, -1), (-1, 0), (1, 0), (-1, 1), (1, 1), (-1, -1), (1, -1)]  # Center, above, below, left, right, top left, top right, bottom left, bottom right\n",
    "    all_time_series = []\n",
    "    dates = None\n",
    "\n",
    "    for offset_x, offset_y in offsets:\n",
    "        current_dates, current_series = unique_extract_time_series_for_coordinate(folder_path, central_coord, offset_x, offset_y, filter, scale)\n",
    "        if dates is None:\n",
    "            dates = current_dates\n",
    "        all_time_series.append(current_series)\n",
    "\n",
    "    return dates, all_time_series\n",
    "\n",
    "# Function to add time series with surrounding pixels to DataFrame\n",
    "def unique_add_time_series_with_surrounding_pixels_to_df(series_dataframe_, gap_folder, attribute_table, filterp=False, scalep=False):\n",
    "    gap_number = int(gap_folder.split('gap')[-1])\n",
    "    gap_coord = read_gap_coordinates(gap_number, attribute_table)\n",
    "    dates, all_coord_values = unique_extract_time_series_for_surrounding_pixels(gap_folder, gap_coord, filter=filterp, scale=scalep)\n",
    "    \n",
    "    labels = ['centre', 'above', 'below', 'left', 'right','top_left', 'top_right', 'bottom_left','bottom_right']\n",
    "    data = {f'gap{gap_number}_{label}': values for label, values in zip(labels, all_coord_values)}\n",
    "    data['Date'] = dates\n",
    "\n",
    "    gap_df = pd.DataFrame(data)\n",
    "    gap_df.set_index('Date', inplace=True)\n",
    "    series_dataframe_ = series_dataframe_.merge(gap_df, left_index=True, right_index=True, how='outer')\n",
    "    \n",
    "    return series_dataframe_\n",
    "\n",
    "# Function to process gap folders and add time series data to DataFrame\n",
    "def unique_process_gap_folders_with_surrounding_pixels(base_folder, df, series_dataframe, filterp=False, scalep=False):\n",
    "    gap_folders = [os.path.join(base_folder, folder) for folder in os.listdir(base_folder) if folder.startswith('gap')]\n",
    "\n",
    "    for gap_folder in gap_folders:\n",
    "        print(gap_folder)\n",
    "        series_dataframe = unique_add_time_series_with_surrounding_pixels_to_df(series_dataframe, gap_folder, df, filterp=filterp, scalep=scalep)\n",
    "\n",
    "    return series_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_series_for_random_points_with_neighbors(folder_path, center_coord, radius, num_points, filter='False', scale=False):\n",
    "    # Generate random coordinates within the specified radius\n",
    "    random_points = generate_random_coordinates(center_coord, radius, num_points)\n",
    "    all_time_series_random = []\n",
    "\n",
    "    # List all TIF files in the folder\n",
    "    tif_files = list_tif_files(folder_path)\n",
    "    dates = []\n",
    "\n",
    "    # Initialize the nested list structure\n",
    "    for i in range(num_points):\n",
    "        all_time_series_random.append([[] for _ in range(9)])\n",
    "\n",
    "    # Extract time series for each random point and its neighbors\n",
    "    for tif_file in tif_files:\n",
    "        date_str = os.path.basename(tif_file).split('-')[-3] + '-' + os.path.basename(tif_file).split('-')[-2] + '-' + os.path.basename(tif_file).split('-')[-1].split('.')[0]\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        dates.append(date)\n",
    "\n",
    "        for i, (utm_x, utm_y) in enumerate(random_points):\n",
    "            offsets = [(0, 0), (0, 1), (0, -1), (-1, 0), (1, 0), (-1, 1), (1, 1), (-1, -1), (1, -1)]  # Center, above, below, left, right, top left, top right, bottom left, bottom right\n",
    "\n",
    "            for j, (offset_x, offset_y) in enumerate(offsets):\n",
    "                coord_value = unique_get_pixel_value(tif_file, utm_x, utm_y, offset_x, offset_y, filter != 'False')\n",
    "                all_time_series_random[i][j].append(coord_value)\n",
    "\n",
    "    if scale:\n",
    "        all_time_series_random = [[[10**(value / 10) for value in series] for series in point_series] for point_series in all_time_series_random]\n",
    "\n",
    "    # Apply the chosen filter\n",
    "    if filter == 'kalman':\n",
    "        for i in range(num_points):\n",
    "            for j in range(5):\n",
    "                kf = KalmanFilter(initial_state_mean=all_time_series_random[i][j][0], n_dim_obs=1)\n",
    "                all_time_series_random[i][j], _ = kf.smooth(all_time_series_random[i][j])\n",
    "                all_time_series_random[i][j] = all_time_series_random[i][j].flatten()\n",
    "    elif filter == 'savgol':\n",
    "        all_time_series_random = [[savgol_filter(series, window_length=5, polyorder=2) for series in point_series] for point_series in all_time_series_random]\n",
    "\n",
    "    return dates, all_time_series_random\n",
    "\n",
    "\n",
    "\n",
    "def unique_add_time_series_for_random_points_to_df(series_dataframe_, gap_folder,attribute_table, radius=50, num_points=3, filterp=False, scalep=False):\n",
    "\n",
    "    gap_number = int(gap_folder.split('gap')[-1])\n",
    "    gap_coord = read_gap_coordinates(gap_number, attribute_table)\n",
    "\n",
    "    ## DOES THE FOLLOWING LINE NEED TO BE HERE:\n",
    "    # I WILL COMMENT IT OUT FOR NOW\n",
    "    # dates, all_coord_values = unique_extract_time_series_for_surrounding_pixels(gap_folder, gap_coord, filter=filterp, scale=scalep)\n",
    "\n",
    "    # Extract the time series data for random points and their neighbors\n",
    "    dates, all_time_series_random = extract_time_series_for_random_points_with_neighbors(gap_folder, gap_coord, radius, num_points, filter=filterp, scale=scalep)\n",
    "    \n",
    "    labels = ['centre', 'above', 'below', 'left', 'right','top_left', 'top_right', 'bottom_left','bottom_right']\n",
    "    \n",
    "    for i in range(num_points):\n",
    "        data = {f'gap{gap_number}_random{i+1}_{label}': values for label, values in zip(labels, all_time_series_random[i])}\n",
    "        data['Date'] = dates\n",
    "        \n",
    "        random_df = pd.DataFrame(data)\n",
    "        random_df.set_index('Date', inplace=True)\n",
    "        series_dataframe_ = series_dataframe_.merge(random_df, left_index=True, right_index=True, how='outer')\n",
    "    \n",
    "    return series_dataframe_\n",
    "\n",
    "\n",
    "def unique_process_gap_folders_with_surrounding_pixels_random(base_folder, df, series_dataframe,radius,num_points, filterp=False, scalep=False):\n",
    "    gap_folders = [os.path.join(base_folder, folder) for folder in os.listdir(base_folder) if folder.startswith('gap')]\n",
    "\n",
    "    for gap_folder in gap_folders:\n",
    "        print(gap_folder)\n",
    "        series_dataframe = unique_add_time_series_for_random_points_to_df(series_dataframe, gap_folder, df,radius=50,num_points=3, filterp=filterp, scalep=scalep)\n",
    "\n",
    "    return series_dataframe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series_2yr_gaps_with_neighbours = unique_process_gap_folders_with_surrounding_pixels('downld_2yr_12dayinterval', df, df_gen1, filterp=False, scalep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series_2yr_random_with_neighbours=unique_process_gap_folders_with_surrounding_pixels_random('downld_2yr_12dayinterval',df,df_gen1,50,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create time series dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## form generic dataframes ##\n",
    "\n",
    "start_date = datetime(2017, 11, 11)\n",
    "end_date = datetime.now()\n",
    "date_range = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "df_gen1 = pd.DataFrame(index=date_range)\n",
    "df_gen2 = pd.DataFrame(index=date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_series_alltime_gaps, df_series_alltime_random = process_gap_folders('downloaded_files', df, df_gen1, df_gen2, filterp=False, scalep=False)\n",
    "\n",
    "#df_series_2yr_gaps, df_series_2yr_random = process_gap_folders('downld_2yr_12dayinterval', df, df_gen1, df_gen2, filterp=False, scalep=False)\n",
    "\n",
    "#df_series_2yr_gaps_filtered_kalman , df_series_2yr_random_filtered_kalman = process_gap_folders('downld_2yr_12dayinterval', df, df_gen1, df_gen2, filterp='kalman', scalep=False)\n",
    "#df_series_2yr_gaps_filtered_savgol , df_series_2yr_random_filtered_savgol = process_gap_folders('downld_2yr_12dayinterval', df, df_gen1, df_gen2, filterp='savgol', scalep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shorten the indefinete / alltime random dataframe so we can compare\n",
    "\n",
    "def keep_first_two_years(df):\n",
    "    df_copy = df.copy()\n",
    "    for column in df_copy.columns:\n",
    "        # Find the first non-NaN entry in the column\n",
    "        first_valid_index = df_copy[column].first_valid_index()\n",
    "        \n",
    "        if first_valid_index is not None:\n",
    "            # Calculate the date two years after the first non-NaN entry\n",
    "            cutoff_date = first_valid_index + pd.DateOffset(years=2)\n",
    "            \n",
    "            # Set all entries after the cutoff date to NaN\n",
    "            df_copy.loc[df_copy.index > cutoff_date, column] = pd.NA\n",
    "\n",
    "    return df_copy\n",
    "\n",
    "# Applying the function to the dataframe\n",
    "#df_series_alltime_random_reduced = keep_first_two_years(df_series_alltime_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse slopes on time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot aggregate statistics for two dataframes on a single plot\n",
    "def plot_aggregates(df1, df2, title1, title2, df3, df4, title3, title4):\n",
    "    # Calculate the mean and standard deviation for the first dataframe set\n",
    "    mean_series1 = df1.mean(axis=1, skipna=True).dropna()\n",
    "    std_series1 = df1.std(axis=1, skipna=True).dropna()\n",
    "    \n",
    "    mean_series2 = df2.mean(axis=1, skipna=True).dropna()\n",
    "    std_series2 = df2.std(axis=1, skipna=True).dropna()\n",
    "    \n",
    "    # Calculate the mean and standard deviation for the second dataframe set\n",
    "    mean_series3 = df3.mean(axis=1, skipna=True).dropna()\n",
    "    std_series3 = df3.std(axis=1, skipna=True).dropna()\n",
    "    \n",
    "    mean_series4 = df4.mean(axis=1, skipna=True).dropna()\n",
    "    std_series4 = df4.std(axis=1, skipna=True).dropna()\n",
    "    \n",
    "    # Plot the results\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6.2, 5.5), sharex=True)\n",
    "    \n",
    "    # Plot for the first dataframe set\n",
    "    ax1.plot(mean_series1, label=f'{title1} Mean', color='#FF0000')\n",
    "    # ax1.fill_between(mean_series1.index, mean_series1 - std_series1, mean_series1 + std_series1, color='blue', alpha=0.2, label=f'{title1} Standard Deviation')\n",
    "    \n",
    "    ax1.plot(mean_series2, label=f'{title2} Mean', color='#0000FF')\n",
    "    # ax1.fill_between(mean_series2.index, mean_series2 - std_series2, mean_series2 + std_series2, color='green', alpha=0.2, label=f'{title2} Standard Deviation')\n",
    "    \n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    ax1.set_ylabel('Pixel Values')\n",
    "    ax1.text(1.02, 0.5, '(a)', transform=ax1.transAxes, verticalalignment='center', fontweight='bold')\n",
    "\n",
    "    \n",
    "    # Plot for the second dataframe set\n",
    "    ax2.plot(mean_series3, label=f'{title3} Mean', color='#FF0000')\n",
    "    # ax2.fill_between(mean_series3.index, mean_series3 - std_series3, mean_series3 + std_series3, color='blue', alpha=0.2, label=f'{title3} Standard Deviation')\n",
    "    \n",
    "    ax2.plot(mean_series4, label=f'{title4} Mean', color='#0000FF')\n",
    "    # ax2.fill_between(mean_series4.index, mean_series4 - std_series4, mean_series4 + std_series4, color='green', alpha=0.2, label=f'{title4} Standard Deviation')\n",
    "    \n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.set_ylabel('Pixel Values')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    ax2.text(1.02, 0.5, '(b)', transform=ax2.transAxes, verticalalignment='center', fontweight='bold')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_aggregates(df_series_alltime_gaps, df_series_alltime_random, 'Indefinite Gaps', 'Indefinite Random',df_series_2yr_gaps, df_series_alltime_random_reduced, '2 year Gaps', '2 year Random')\n",
    "#plt.savefig('series_lineplot_double.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering and Time Series Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Filter Time Series Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to eliminate neighbour points from dataframes for comparison\n",
    "def rename_for_compare(dfx):\n",
    "\n",
    "    df = dfx.copy()\n",
    "    # Filter columns to keep only those with 'centre'\n",
    "    centre_cols = [col for col in df.columns if 'centre' in col]\n",
    "    df_centre_only = df[centre_cols]\n",
    "\n",
    "    # Rename columns to remove '_centre' part\n",
    "    new_col_names = [col.replace('_centre', '') for col in df_centre_only.columns]\n",
    "    df_centre_only.columns = new_col_names\n",
    "\n",
    "    return df_centre_only\n",
    "\n",
    "\n",
    "def filter_and_rename_columns(df):\n",
    "    # Filter columns to keep only those with '_centre'\n",
    "    centre_columns = [col for col in df.columns if col.endswith('_centre')]\n",
    "    \n",
    "    # Rename columns by removing '_centre'\n",
    "    new_column_names = {col: col.replace('_centre', '') for col in centre_columns}\n",
    "    \n",
    "    # Select the filtered columns and rename them\n",
    "    filtered_df = df[centre_columns].rename(columns=new_column_names)\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def calculate_image_variance(file_path):\n",
    "    \"\"\"Load the image from the file path and calculate the variance of the pixel values.\"\"\"\n",
    "    with rasterio.open(file_path) as src:\n",
    "        image = src.read(1)  # Read the first band\n",
    "    return np.var(image)\n",
    "\n",
    "## ALL FILTERING FUNCTIONS\n",
    "\n",
    "def create_datacube(df, gap_number,random_id=0, gap=True):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame with 9 columns into a 3D array for filtering for a specific gap number.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame with SAR data having multiple gaps or random points.\n",
    "    gap_number (int): The number of the gap or random points columns to be extracted (e.g., 1, 5).\n",
    "    gap (bool): If True, handles gap points; if False, handles random points.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: 3D array (3x3xt) where t is the number of rows in the DataFrame.\n",
    "    \"\"\"\n",
    "    if gap:\n",
    "        # Define the correct order of the columns for the specified gap\n",
    "        column_order = [\n",
    "            f'gap{gap_number}_top_left', f'gap{gap_number}_above', f'gap{gap_number}_top_right', \n",
    "            f'gap{gap_number}_left', f'gap{gap_number}_centre', f'gap{gap_number}_right', \n",
    "            f'gap{gap_number}_bottom_left', f'gap{gap_number}_below', f'gap{gap_number}_bottom_right'\n",
    "        ]\n",
    "    else:\n",
    "        # Define the correct order of the columns for the specified random points\n",
    "        column_order = [\n",
    "            f'gap{gap_number}_random{random_id}_top_left', f'gap{gap_number}_random{random_id}_above', f'gap{gap_number}_random{random_id}_top_right', \n",
    "            f'gap{gap_number}_random{random_id}_left', f'gap{gap_number}_random{random_id}_centre', f'gap{gap_number}_random{random_id}_right', \n",
    "            f'gap{gap_number}_random{random_id}_bottom_left', f'gap{gap_number}_random{random_id}_below', f'gap{gap_number}_random{random_id}_bottom_right'\n",
    "        ]\n",
    "    \n",
    "    # Ensure the DataFrame has the required columns\n",
    "    for col in column_order:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"The DataFrame does not have the required column: {col}\")\n",
    "    \n",
    "    # Number of time steps\n",
    "    t = df.shape[0]\n",
    "    \n",
    "    # Initialize the 3D array\n",
    "    datacube = np.zeros((3, 3, t))\n",
    "    \n",
    "    # Populate the 3D array\n",
    "    for i in range(t):\n",
    "        values = df.iloc[i][column_order].values\n",
    "        datacube[:, :, i] = values.reshape((3, 3))\n",
    "    \n",
    "    return datacube\n",
    "\n",
    "\n",
    "def dB_to_linear(db_array):\n",
    "    \"\"\"Convert dB to linear scale.\"\"\"\n",
    "    return 10 ** (db_array / 10)\n",
    "\n",
    "def linear_to_dB(linear_array):\n",
    "    \"\"\"Convert linear scale to dB.\"\"\"\n",
    "    ans = 10 * np.log10(linear_array)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def frost_filter(data, damping_factor=0.05):\n",
    "    \"\"\"Apply Frost filter to a 2D array with time series in rows and 9 columns for neighbors.\"\"\"\n",
    "    filtered_data = np.zeros(data.shape[0])\n",
    "    pad_size = 1\n",
    "    padded_data = np.pad(data, ((pad_size, pad_size), (0, 0)), mode='edge')\n",
    "\n",
    "    for t in range(data.shape[0]):\n",
    "        local_window = padded_data[t:t + 3, :]\n",
    "        local_mean = np.mean(local_window, axis=0)\n",
    "        local_variance = np.var(local_window, axis=0)\n",
    "        local_variance = np.clip(local_variance, a_min=1e-8, a_max=None)  # Avoid division by zero\n",
    "        center_value = data[t, :]\n",
    "        \n",
    "        # Calculate coefficients\n",
    "        coefficients = np.exp(-damping_factor * np.abs(local_window - center_value) * local_variance / local_mean**2)\n",
    "        weights = coefficients / np.sum(coefficients, axis=0)\n",
    "        \n",
    "        # Apply weights to the local window to get the filtered value\n",
    "        filtered_data[t] = np.sum(weights * local_window, axis=0)[0]\n",
    "\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def lee_filter(data):\n",
    "    \"\"\"Apply Lee filter to a 2D array with time series in rows and 9 columns for neighbors.\"\"\"\n",
    "    column_name = data.columns[0]\n",
    "    gap_name = column_name.split('_')[0]\n",
    "\n",
    "    filtered_data = np.zeros(data.shape[0])\n",
    "    pad_size = 1\n",
    "    padded_data = np.pad(data.values, ((pad_size, pad_size), (0, 0)), mode='edge')\n",
    "\n",
    "    for t, date in enumerate(data.index):\n",
    "\n",
    "        # Construct the file path\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        file_path = f'downld_2yr_12dayinterval/{gap_name}/{gap_name}-{date_str}.tif'\n",
    "        \n",
    "        # Verify the file path and calculate the image variance\n",
    "        if not os.path.isfile(file_path):\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            noise_variance = np.var(data.values)  # Fallback to variance from the data if file not found\n",
    "        else:\n",
    "            noise_variance = calculate_image_variance(file_path)\n",
    "\n",
    "        local_window = padded_data[t:t + 3, :]\n",
    "        local_mean = np.mean(local_window, axis=0)\n",
    "        local_variance = np.var(local_window, axis=0)\n",
    "        k = local_variance / (local_variance + noise_variance)\n",
    "        filtered_data[t] = local_mean[0] + k[0] * (data.values[t, 0] - local_mean[0])\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def adaptive_wiener_filter(data):\n",
    "    \"\"\"Apply Adaptive Wiener filter to a 2D array with time series in rows and 9 columns for neighbors.\"\"\"\n",
    "\n",
    "    \n",
    "    filtered_data = np.zeros(data.shape[0])\n",
    "    pad_size = 1\n",
    "    padded_data = np.pad(data, ((pad_size, pad_size), (0, 0)), mode='edge')\n",
    "\n",
    "    for t in range(data.shape[0]):\n",
    "        local_window = padded_data[t:t + 3, :]\n",
    "        local_mean = np.mean(local_window, axis=0)\n",
    "        local_variance = np.var(local_window, axis=0)\n",
    "        noise_variance = np.var(data)\n",
    "        k = local_variance / (local_variance + noise_variance)\n",
    "        filtered_data[t] = local_mean[0] + k[0] * (data[t, 0] - local_mean[0])\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def quegan_filter(dataframe,gap_number,random_id=0,gap=True):\n",
    "\n",
    "\n",
    "    datacube = create_datacube(dataframe, gap_number,random_id, gap)\n",
    "    \n",
    "    # Calculate temporal mean for each pixel\n",
    "    temporal_mean = np.mean(datacube, axis=2)[:,:,np.newaxis]\n",
    "    \n",
    "    # Calculate temporal variance for each pixel\n",
    "    temporal_var = np.var(datacube, axis=2)[:,:,np.newaxis]\n",
    "    \n",
    "    # Calculate filter strength\n",
    "    k = 1 - (temporal_var / (temporal_mean ** 2 + temporal_var))\n",
    "    \n",
    "    # Apply filter to all time steps at once\n",
    "    filtered_cube = k * temporal_mean + (1 - k) * datacube\n",
    "\n",
    "\n",
    "    return filtered_cube[1,1,:]\n",
    "\n",
    "\n",
    "def apply_median_temporal_filter(df, window_size=3):\n",
    "    \"\"\"Apply a median temporal filter to the time series in the dataframe.\"\"\"\n",
    "    filtered_df = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Convert from dB to linear scale\n",
    "        time_series = df[col].values\n",
    "        \n",
    "        # Drop NaNs and keep the index for restoring later\n",
    "        non_nan_series = df[col].dropna()\n",
    "        non_nan_index = non_nan_series.index\n",
    "        non_nan_values = dB_to_linear(non_nan_series.values)\n",
    "        \n",
    "        # Apply the median filter\n",
    "        filtered_non_nan_values = median_filter(non_nan_values, size=window_size)\n",
    "        \n",
    "        # Convert back to dB scale\n",
    "        filtered_non_nan_db = linear_to_dB(filtered_non_nan_values)\n",
    "        \n",
    "        # Create a series to hold the filtered values, reintroducing NaNs where appropriate\n",
    "        filtered_series = pd.Series(data=np.nan, index=df.index)\n",
    "        filtered_series[non_nan_index] = filtered_non_nan_db\n",
    "        \n",
    "        # Store the filtered time series in the output DataFrame\n",
    "        filtered_df[col] = filtered_series\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def quegan_filter3d(dataframe, gap_number, random_id=0, gap=True, time_window_size=3):\n",
    "    \"\"\"\n",
    "    Apply 3D Quegan filter to the datacube and return the filtered center pixel values.\n",
    "    \n",
    "    Parameters:\n",
    "    dataframe (pd.DataFrame): Input DataFrame with SAR data.\n",
    "    gap_number (int): The number of the gap or random points columns to be extracted.\n",
    "    random_id (int): The random point id to be used if gap is False.\n",
    "    gap (bool): If True, handles gap points; if False, handles random points.\n",
    "    time_window_size (int): Size of the time window for filtering (default: 3).\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: 1D array of filtered center pixel values.\n",
    "    \"\"\"\n",
    "    \n",
    "    datacube = create_datacube(dataframe, gap_number, random_id, gap)\n",
    "    \n",
    "    # Convert the datacube from dB to linear scale for processing\n",
    "    datacube_linear = dB_to_linear(datacube)\n",
    "    \n",
    "    spatial_window_size = 3\n",
    "    pad_width_spatial = spatial_window_size // 2\n",
    "    pad_width_time = time_window_size // 2\n",
    "    \n",
    "    padded_cube = np.pad(datacube_linear, ((pad_width_spatial,), (pad_width_spatial,), (pad_width_time,)), mode='edge')\n",
    "    \n",
    "    filtered_cube = np.zeros_like(datacube_linear)\n",
    "    \n",
    "    # Use numpy's stride tricks for efficient sliding window\n",
    "    from numpy.lib.stride_tricks import sliding_window_view\n",
    "    windows = sliding_window_view(padded_cube, (spatial_window_size, spatial_window_size, time_window_size))\n",
    "    \n",
    "    for i in tqdm(range(datacube.shape[0])):\n",
    "        for j in range(datacube.shape[1]):\n",
    "            for k in range(datacube.shape[2]):\n",
    "                local_window = windows[i, j, k]\n",
    "                local_mean = np.mean(local_window)\n",
    "                local_var = np.var(local_window)\n",
    "                \n",
    "                if local_var == 0 and local_mean == 0:\n",
    "                    k_value = 0\n",
    "                else:\n",
    "                    k_value = 1 - (local_var / (local_var + local_mean**2))\n",
    "                \n",
    "                filtered_cube[i, j, k] = k_value * local_mean + (1 - k_value) * datacube_linear[i, j, k]\n",
    "    \n",
    "    # Convert the filtered datacube back to dB scale\n",
    "    filtered_cube_dB = linear_to_dB(filtered_cube)\n",
    "\n",
    "    \n",
    "    return filtered_cube_dB[1, 1, :]\n",
    "\n",
    "\n",
    "def quegan_filter3d(dataframe,gap_number,random_id,gap=True, spatial_window=3):\n",
    "    \"\"\"\n",
    "    Apply a spatiotemporal Quegan filter to the datacube.\n",
    "    \n",
    "    Parameters:\n",
    "    datacube (np.ndarray): Input 3D array with shape (height, width, time)\n",
    "    spatial_window (int): Size of the spatial window (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: Filtered datacube\n",
    "    \"\"\"\n",
    "    \n",
    "    datacube = create_datacube(dataframe, gap_number,random_id, gap)\n",
    "\n",
    "    filtered_cube = np.zeros_like(datacube, dtype=float)\n",
    "    \n",
    "    # Calculate spatiotemporal mean\n",
    "    spatiotemporal_mean = uniform_filter(datacube, size=(spatial_window, spatial_window, datacube.shape[2]))\n",
    "    \n",
    "    # Calculate spatiotemporal variance\n",
    "    spatiotemporal_var = uniform_filter(datacube**2, size=(spatial_window, spatial_window, datacube.shape[2])) - spatiotemporal_mean**2\n",
    "    \n",
    "    # Ensure variance is non-negative (floating point precision issues)\n",
    "    spatiotemporal_var = np.maximum(spatiotemporal_var, 0)\n",
    "    \n",
    "    # Calculate filter strength\n",
    "    k = 1 - (spatiotemporal_var / (spatiotemporal_mean**2 + spatiotemporal_var))\n",
    "    \n",
    "    # Apply filter\n",
    "    filtered_cube = k * spatiotemporal_mean + (1 - k) * datacube\n",
    "    \n",
    "    return filtered_cube[1,1,:]\n",
    "\n",
    "\n",
    "def quegan_frost(dataframe, gap_number, random_id=0, gap=True, damping_factor=1):\n",
    "    datacube = create_datacube(dataframe, gap_number, random_id, gap)\n",
    "    \n",
    "    # Calculate temporal mean for each pixel\n",
    "    temporal_mean = np.mean(datacube, axis=2)[:, :, np.newaxis]\n",
    "    \n",
    "    # Calculate temporal variance for each pixel\n",
    "    temporal_var = np.var(datacube, axis=2)[:, :, np.newaxis]\n",
    "    \n",
    "    # Calculate filter strength\n",
    "    k = 1 - (temporal_var / (temporal_mean ** 2 + temporal_var))\n",
    "    \n",
    "    # Apply filter to all time steps at once\n",
    "    data = k * temporal_mean + (1 - k) * datacube\n",
    "\n",
    "    # Initialize filtered data array\n",
    "    filtered_data = np.zeros(data.shape)\n",
    "    \n",
    "    # Pad data for edge handling\n",
    "    pad_size = 1\n",
    "    padded_data = np.pad(data, ((pad_size, pad_size), (pad_size, pad_size), (0, 0)), mode='edge')\n",
    "\n",
    "    # Iterate through spatial dimensions and time\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            for t in range(data.shape[2]):\n",
    "                # Extract local window\n",
    "                local_window = padded_data[i:i + 3, j:j + 3, t]\n",
    "                center_value = data[i, j, t]\n",
    "                \n",
    "                # Calculate local mean and variance\n",
    "                local_mean = np.mean(local_window)\n",
    "                local_variance = np.var(local_window)\n",
    "                local_variance = np.clip(local_variance, a_min=1e-8, a_max=None)  # Avoid division by zero\n",
    "                \n",
    "                # Calculate coefficients\n",
    "                coefficients = np.exp(-damping_factor * np.abs(local_window - center_value) * local_variance / local_mean**2)\n",
    "                weights = coefficients / np.sum(coefficients)\n",
    "                \n",
    "                # Apply weights to the local window to get the filtered value\n",
    "                filtered_data[i, j, t] = np.sum(weights * local_window)\n",
    "\n",
    "    return filtered_data[1,1,:]\n",
    "\n",
    "\n",
    "\n",
    "def process_dataframe(df, labels, filter_type):\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*DataFrame is highly fragmented.*\")\n",
    "    df_filtered = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for gap_id in range(1, 108):\n",
    "        cols = [f'gap{gap_id}_{label}' for label in labels]\n",
    "        sub_df = df[cols]\n",
    "        \n",
    "        if sub_df.isnull().all().all():\n",
    "            continue\n",
    "        \n",
    "        sub_df_linear = dB_to_linear(sub_df)\n",
    "        sub_df_linear.dropna(inplace=True)\n",
    "        \n",
    "        if sub_df_linear.empty:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        if filter_type == 'frost':\n",
    "            filtered_linear = frost_filter(sub_df_linear.values)\n",
    "        elif filter_type == 'lee':\n",
    "            filtered_linear = lee_filter(sub_df_linear)\n",
    "        elif filter_type == 'quegan':\n",
    "            filtered_linear = quegan_filter(sub_df_linear,gap_id,0,True)\n",
    "        elif filter_type == 'quegan3d':\n",
    "            filtered_linear = quegan_filter3d(sub_df_linear,gap_id,0,True)\n",
    "        elif filter_type == 'quegan_frost':\n",
    "            filtered_linear = quegan_frost(sub_df_linear,gap_id,0,True)\n",
    "\n",
    "        \n",
    "        filtered_db = linear_to_dB(filtered_linear)\n",
    "        filtered_series = pd.Series(filtered_db, index=sub_df_linear.index)\n",
    "        df_filtered[f'gap{gap_id}_centre'] = filtered_series\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def process_random_dataframe(df, labels, filter_type):\n",
    "    warnings.filterwarnings(\"ignore\", message=\".*DataFrame is highly fragmented.*\")\n",
    "    df_filtered = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for gap_id in range(1, 108):\n",
    "        for random_id in range(1, 4):\n",
    "            cols = [f'gap{gap_id}_random{random_id}_{label}' for label in labels]\n",
    "            sub_df = df[cols]\n",
    "            \n",
    "            if sub_df.isnull().all().all():\n",
    "                continue\n",
    "\n",
    "            \n",
    "            sub_df_linear = dB_to_linear(sub_df)\n",
    "            sub_df_linear.dropna(inplace=True)\n",
    "            \n",
    "            if sub_df_linear.empty:\n",
    "                continue\n",
    "            \n",
    "            if filter_type == 'frost':\n",
    "                filtered_linear = frost_filter(sub_df_linear.values)\n",
    "            elif filter_type == 'lee':\n",
    "                filtered_linear = lee_filter(sub_df_linear)\n",
    "            elif filter_type == 'quegan':\n",
    "                filtered_linear = quegan_filter(sub_df_linear, gap_id, random_id, False)\n",
    "            elif filter_type == 'quegan3d':\n",
    "                filtered_linear = quegan_filter3d(sub_df_linear, gap_id, random_id, False)\n",
    "            elif filter_type == 'quegan_frost':\n",
    "                filtered_linear = quegan_frost(sub_df_linear, gap_id, random_id, False)\n",
    "            \n",
    "            filtered_db = linear_to_dB(filtered_linear)\n",
    "            filtered_series = pd.Series(filtered_db, index=sub_df_linear.index)\n",
    "            df_filtered[f'gap{gap_id}_random{random_id}_centre'] = filtered_series\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATION OF FILTERED DATAFRAMES\n",
    "'''\n",
    "IN GENERAL:\n",
    "start with dataframes with neighbours: df_series_2yr_gaps_with_neighbours, df_series_2yr_random_with_neighbours\n",
    "then put them through the relevant filters.\n",
    "Changes slightly for temporal filters since neighbours arent needed\n",
    "'''\n",
    "\n",
    "labels = ['centre', 'above', 'below', 'left', 'right', 'top_left', 'top_right', 'bottom_left', 'bottom_right']\n",
    "\n",
    "\n",
    "## COMPARISON / INITIAL FILTERS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "df_series_2yr_gaps = rename_for_compare(df_series_2yr_gaps_with_neighbours)\n",
    "df_series_2yr_random = rename_for_compare(df_series_2yr_random_with_neighbours)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print('comparison done')\n",
    "\n",
    "## FROST FILTERS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "df_frost_gaps = process_dataframe(df_series_2yr_gaps_with_neighbours, labels, 'frost')\n",
    "df_frost_gaps = df_frost_gaps.reindex(df_gen1.index)\n",
    "\n",
    "df_frost_random=process_random_dataframe(df_series_2yr_random_with_neighbours,labels, 'frost')\n",
    "df_frost_random = df_frost_random.reindex(df_gen1.index)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print('frost done')\n",
    "\n",
    "## LEE FILTERS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "df_lee_gaps = process_dataframe(df_series_2yr_gaps_with_neighbours, labels,'lee')\n",
    "df_lee_gaps = df_lee_gaps.reindex(df_gen1.index)\n",
    "\n",
    "df_lee_random=process_random_dataframe(df_series_2yr_random_with_neighbours,labels, 'lee')\n",
    "df_lee_random = df_lee_random.reindex(df_gen1.index)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print('lee done')\n",
    "\n",
    "## TEMPORAL FILTERS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "df_temporal_gaps = rename_for_compare(df_series_2yr_gaps_with_neighbours)\n",
    "df_temporal_gaps = apply_median_temporal_filter(df_temporal_gaps)\n",
    "df_temporal_gaps = df_temporal_gaps.reindex(df_gen1.index)\n",
    "\n",
    "df_series_2yr_random_for_filter_comparison = rename_for_compare(df_series_2yr_random_with_neighbours)\n",
    "df_temporal_random = apply_median_temporal_filter(df_series_2yr_random_for_filter_comparison)\n",
    "df_temporal_random = df_temporal_random.reindex(df_gen1.index)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print('temporal done')\n",
    "\n",
    "## TEMPORAL LEE FILTERS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "# apply temporal filter to whole neighbours time series, then apply lee filter\n",
    "df_temporal_lee_gaps=df_series_2yr_gaps_with_neighbours.copy()\n",
    "df_temporal_lee_random = df_series_2yr_random_with_neighbours.copy()\n",
    "\n",
    "df_temporal_lee_gaps = apply_median_temporal_filter(df_temporal_lee_gaps)\n",
    "df_temporal_lee_gaps = process_dataframe(df_temporal_lee_gaps, labels, 'lee')\n",
    "df_temporal_lee_gaps = df_temporal_lee_gaps.reindex(df_gen1.index)\n",
    "\n",
    "df_temporal_lee_random=apply_median_temporal_filter(df_temporal_lee_random)\n",
    "df_temporal_lee_random=process_random_dataframe(df_temporal_lee_random,labels,'lee')\n",
    "df_temporal_lee_random=df_temporal_lee_random.reindex(df_gen1.index)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print('temporal lee done')\n",
    "\n",
    "\n",
    "## LEE TEMPORAL FILTERS~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "# apply temporal filter on already filtered lee filter time series\n",
    "#df_lee_temporal_gaps=apply_median_temporal_filter(df_lee_gaps.copy())\n",
    "#df_lee_temporal_random=apply_median_temporal_filter(df_lee_random.copy())\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "#print('lee temporal done')\n",
    "\n",
    "\n",
    "## TEMPORAL FROST~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "df_temporal_frost_gaps=df_series_2yr_gaps_with_neighbours.copy()\n",
    "df_temporal_frost_random=df_series_2yr_random_with_neighbours.copy()\n",
    "\n",
    "df_temporal_frost_gaps = apply_median_temporal_filter(df_temporal_frost_gaps)\n",
    "df_temporal_frost_gaps = process_dataframe(df_temporal_frost_gaps, labels, 'frost').reindex(df_gen1.index)\n",
    "\n",
    "df_temporal_frost_random = apply_median_temporal_filter(df_temporal_frost_random)\n",
    "df_temporal_frost_random = process_random_dataframe(df_temporal_frost_random, labels, 'frost').reindex(df_gen1.index)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print('temporal frost done')\n",
    "\n",
    "\n",
    "## QUEGAN FILTER~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "df_quegan_gaps=process_dataframe(df_series_2yr_gaps_with_neighbours,labels,'quegan').reindex(df_gen1.index)\n",
    "df_quegan_random=process_random_dataframe(df_series_2yr_random_with_neighbours,labels,'quegan').reindex(df_gen1.index)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print('quegan done')\n",
    "\n",
    "\n",
    "## QUEGAN FROST FILTER~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "## NB ITS LABELED TEMPORAL QUEGAN\n",
    "df_quegan_frost_gaps=df_series_2yr_gaps_with_neighbours.copy()\n",
    "df_quegan_frost_random = df_series_2yr_random_with_neighbours.copy()\n",
    "\n",
    "df_quegan_frost_gaps = process_dataframe(df_quegan_frost_gaps, labels, 'quegan_frost').reindex(df_gen1.index)\n",
    "df_quegan_frost_random=process_random_dataframe(df_quegan_frost_random,labels,'quegan_frost').reindex(df_gen1.index)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print('quegan frost done')\n",
    "\n",
    "\n",
    "## QUEGAN 3d FILTER~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~##\n",
    "df_quegan3d_gaps=process_dataframe(df_series_2yr_gaps_with_neighbours,labels,'quegan3d').reindex(df_gen1.index)\n",
    "df_quegan3d_random=process_random_dataframe(df_series_2yr_random_with_neighbours,labels,'quegan3d').reindex(df_gen1.index)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print('quegan 3d done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df, label):\n",
    "    features = []\n",
    "    for col in df.columns:\n",
    "        series = df[col].dropna()  # Drop NaNs for feature calculation\n",
    "        if not series.empty:\n",
    "            mean_val = series.mean()\n",
    "            var_val = series.var()\n",
    "            skew_val = skew(series)\n",
    "            max_val = series.max()\n",
    "            min_val = series.min()\n",
    "            range_val = max_val - min_val\n",
    "\n",
    "            features.append({\n",
    "                'name': col,\n",
    "                'mean': mean_val,\n",
    "                'variance': var_val,\n",
    "                'skewness': skew_val,\n",
    "                'max': max_val,\n",
    "                'min': min_val,\n",
    "                'range': range_val,\n",
    "                'gap': label\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(features)\n",
    "\n",
    "def create_feature_matrix(gap_df, random_df, area_df):\n",
    "    # Make copies of the dataframes to avoid modifying the original ones\n",
    "    gap_df = gap_df.copy()\n",
    "    random_df = random_df.copy()\n",
    "    area_df = area_df.copy()\n",
    "\n",
    "    # Extract features for both dataframes\n",
    "    gap_features = extract_features(gap_df, label=1)\n",
    "    random_features = extract_features(random_df, label=0)\n",
    "\n",
    "    # Add a zero-indexed row number to area_df\n",
    "    area_df['row_index'] = area_df.index\n",
    "\n",
    "    # Extract the numeric part of the 'name' column from gap_features\n",
    "    gap_features['name_index'] = gap_features['name'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "    # Merge based on the row number\n",
    "    gap_features = gap_features.merge(area_df[['row_index', 'area']], left_on='name_index', right_on='row_index', how='left')\n",
    "    gap_features = gap_features.drop(columns=['row_index', 'name_index'])\n",
    "\n",
    "    # Add area column to random_features with NaN values\n",
    "    random_features['area'] = np.nan\n",
    "\n",
    "    # Combine the features into a single DataFrame\n",
    "    X_df = pd.concat([gap_features, random_features], ignore_index=True)\n",
    "\n",
    "    return X_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOW CREATE FEATURE DATAFRAMES AFTER FILTERED TIME SERIES HAVE BEEN CREATED\n",
    "\n",
    "#X_df_alltime = create_feature_matrix(df_series_alltime_gaps, df_series_alltime_random, df)\n",
    "\n",
    "X_df_2yr = create_feature_matrix(df_series_2yr_gaps, df_series_2yr_random, df)\n",
    "\n",
    "#X_df_2yr_reduced = create_feature_matrix(df_series_2yr_gaps,df_series_alltime_random_reduced, df)\n",
    "\n",
    "X_df_2yr_frost = create_feature_matrix(df_frost_gaps, df_frost_random,df)\n",
    "\n",
    "X_df_2yr_lee = create_feature_matrix(df_lee_gaps, df_lee_random,df)\n",
    "\n",
    "X_df_2yr_temporal = create_feature_matrix(df_temporal_gaps,df_temporal_random,df)\n",
    "\n",
    "X_df_2yr_temporal_lee = create_feature_matrix(df_temporal_lee_gaps, df_temporal_lee_random,df)\n",
    "\n",
    "#X_df_2yr_lee_temporal = create_feature_matrix(df_lee_temporal_gaps,df_lee_temporal_random,df)\n",
    "\n",
    "#X_df_2yr_frost_temporal = create_feature_matrix(df_)\n",
    "\n",
    "X_df_2yr_temporal_frost = create_feature_matrix(df_temporal_frost_gaps,df_temporal_frost_random,df)\n",
    "\n",
    "X_df_2yr_quegan = create_feature_matrix(df_quegan_gaps,df_quegan_random,df)\n",
    "\n",
    "X_df_2yr_quegan_frost = create_feature_matrix(df_quegan_frost_gaps, df_quegan_frost_random, df)\n",
    "\n",
    "X_df_2yr_quegan3d = create_feature_matrix(df_quegan3d_gaps, df_quegan3d_random, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_distributions_strip(dfs, y_labels):\n",
    "    # Select the columns to plot\n",
    "    features = ['mean', 'variance', 'skewness', 'max', 'min', 'range']\n",
    "    \n",
    "    # Define the custom color palette\n",
    "    palette = {0: 'blue', 1: 'red'}\n",
    "    \n",
    "    # Define the custom labels for the x-axis\n",
    "    x_labels = {\n",
    "        'mean': 'Mean',\n",
    "        'variance': 'Variance',\n",
    "        'skewness': 'Skew',\n",
    "        'max': 'Max',\n",
    "        'min': 'Min',\n",
    "        'range': 'Range'\n",
    "    }\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    num_dfs = len(dfs)\n",
    "    fig, axes = plt.subplots(num_dfs, len(features), figsize=(6.15, 1.1 * num_dfs), sharex='col', sharey='row')\n",
    "    \n",
    "    # Plot histograms for each feature in each dataframe\n",
    "    for row, (df__, y_label) in enumerate(zip(dfs, y_labels)):\n",
    "        for col, feature in enumerate(features):\n",
    "            sns.histplot(data=df__[df__['gap'] == 0], x=feature, kde=True, ax=axes[row, col], color='blue', alpha=0.6, label='0')\n",
    "            sns.histplot(data=df__[df__['gap'] == 1], x=feature, kde=True, ax=axes[row, col], color='red', alpha=0.6, label='1')\n",
    "            \n",
    "            if row == num_dfs - 1:\n",
    "                axes[row, col].set_xlabel(x_labels[feature])  # Set the custom x-axis label for the bottom row\n",
    "                axes[row, col].tick_params(axis='x', which='both', bottom=True)  # Show x-ticks only on the bottom row\n",
    "            else:\n",
    "                axes[row, col].set_xlabel('')\n",
    "                axes[row, col].tick_params(axis='x', which='both', bottom=False)  # Hide x-ticks on other rows\n",
    "            \n",
    "            if col == 0:\n",
    "                axes[row, col].set_ylabel(y_label)  # Set the custom y-axis label for each row\n",
    "                axes[row, col].tick_params(axis='y', which='both', left=True)  # Show y-ticks only on the leftmost column\n",
    "            else:\n",
    "                axes[row, col].set_ylabel('')\n",
    "                axes[row, col].tick_params(axis='y', which='both', left=False)  # Hide y-ticks on other columns\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.07, hspace=0.1)  # Adjust the space between the plots\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your DataFrames are named X_df_2yr, X_df_2yr_frost, X_df_2yr_lee, X_df_2yr_temporal, X_df_2yr_quegan, and X_df_2yr_quegan3d\n",
    "X_df_2yr_list = [X_df_2yr, X_df_2yr_frost, X_df_2yr_lee, X_df_2yr_temporal, X_df_2yr_quegan, X_df_2yr_quegan3d]\n",
    "y_labels = ['None', 'Frost', 'Lee', 'M. Temp.', 'Quegan Temp.', 'Quegan 3D']\n",
    "plot_feature_distributions_strip(X_df_2yr_list, y_labels)\n",
    "plt.savefig('distribution_strip.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_var_axis1(df):\n",
    "\n",
    "    '''\n",
    "    calculate mean and variance for each date\n",
    "    effectively the 'average' time series\n",
    "    '''\n",
    "\n",
    "    mean=df.mean(axis=1)\n",
    "    variance=df.var(axis=1)\n",
    "\n",
    "    return(mean,variance)\n",
    "\n",
    "\n",
    "def mean_var_axis0(df):\n",
    "    \n",
    "    '''\n",
    "    calculates mean and variance for each time series\n",
    "    acts down columns\n",
    "    '''\n",
    "\n",
    "    mean=df.mean(axis=0)\n",
    "    variance=df.var(axis=0)\n",
    "\n",
    "    return(mean,variance)\n",
    "\n",
    "\n",
    "def aggregate_and_ensemble_vals(df):\n",
    "    \n",
    "    ### calculate 'average' time series\n",
    "    m1,v1=mean_var_axis0(df)\n",
    "    ensemble_mean=np.mean(m1)\n",
    "    ensemble_var=np.var(m1)\n",
    "\n",
    "    ### calculate mean of each time series\n",
    "    m2,v2=mean_var_axis1(df)\n",
    "    aggregate_mean=np.mean(m2)\n",
    "    aggregate_var=np.var(m2)\n",
    "\n",
    "    return((ensemble_mean,ensemble_var),(aggregate_mean,aggregate_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_series_2yr_random1 = pd.DataFrame()\n",
    "df_series_2yr_random2 = pd.DataFrame()\n",
    "df_series_2yr_random3 = pd.DataFrame()\n",
    "\n",
    "df_series_2yr_random1_frost = pd.DataFrame()\n",
    "df_series_2yr_random2_frost = pd.DataFrame()\n",
    "df_series_2yr_random3_frost = pd.DataFrame()\n",
    "\n",
    "df_series_2yr_random1_lee = pd.DataFrame()\n",
    "df_series_2yr_random2_lee = pd.DataFrame()\n",
    "df_series_2yr_random3_lee = pd.DataFrame()\n",
    "\n",
    "df_series_2yr_random1_temporal = pd.DataFrame()\n",
    "df_series_2yr_random2_temporal = pd.DataFrame()\n",
    "df_series_2yr_random3_temporal = pd.DataFrame()\n",
    "\n",
    "df_series_2yr_random1_temporal_lee = pd.DataFrame()\n",
    "df_series_2yr_random2_temporal_lee = pd.DataFrame()\n",
    "df_series_2yr_random3_temporal_lee = pd.DataFrame()\n",
    "\n",
    "df_series_2yr_random1_temporal_frost = pd.DataFrame()\n",
    "df_series_2yr_random2_temporal_frost = pd.DataFrame()\n",
    "df_series_2yr_random3_temporal_frost = pd.DataFrame()\n",
    "\n",
    "df_series_2yr_random1_quegan = pd.DataFrame()\n",
    "df_series_2yr_random2_quegan = pd.DataFrame()\n",
    "df_series_2yr_random3_quegan = pd.DataFrame()\n",
    "\n",
    "df_series_2yr_random1_quegan_frost = pd.DataFrame()\n",
    "df_series_2yr_random2_quegan_frost = pd.DataFrame()\n",
    "df_series_2yr_random3_quegan_frost = pd.DataFrame()\n",
    "\n",
    "df_series_2yr_random1_quegan3d = pd.DataFrame()\n",
    "df_series_2yr_random2_quegan3d = pd.DataFrame()\n",
    "df_series_2yr_random3_quegan3d = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Separate columns based on their suffix (random1, random2, random3)\n",
    "for col in df_series_2yr_random_for_filter_comparison.columns:\n",
    "    if col.endswith('_random1'):\n",
    "        df_series_2yr_random1[col] = df_series_2yr_random_for_filter_comparison[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random2'):\n",
    "        df_series_2yr_random2[col] = df_series_2yr_random_for_filter_comparison[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random3'):\n",
    "        df_series_2yr_random3[col] = df_series_2yr_random_for_filter_comparison[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Separate columns based on their suffix (random1, random2, random3)\n",
    "for col in df_frost_random.columns:\n",
    "    if col.endswith('_random1_centre'):\n",
    "        df_series_2yr_random1_frost[col] = df_frost_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random2_centre'):\n",
    "        df_series_2yr_random2_frost[col] = df_frost_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random3_centre'):\n",
    "        df_series_2yr_random3_frost[col] = df_frost_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Separate columns based on their suffix (random1, random2, random3)\n",
    "for col in df_lee_random.columns:\n",
    "    if col.endswith('_random1_centre'):\n",
    "        df_series_2yr_random1_lee[col] = df_lee_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random2_centre'):\n",
    "        df_series_2yr_random2_lee[col] = df_lee_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random3_centre'):\n",
    "        df_series_2yr_random3_lee[col] = df_lee_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "for col in df_temporal_random.columns:\n",
    "    if col.endswith('_random1'):\n",
    "        df_series_2yr_random1_temporal[col] = df_temporal_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random2'):\n",
    "        df_series_2yr_random2_temporal[col] = df_temporal_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random3'):\n",
    "        df_series_2yr_random3_temporal[col] = df_temporal_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "for col in df_temporal_lee_random.columns:\n",
    "    if col.endswith('_random1_centre'):\n",
    "        df_series_2yr_random1_temporal_lee[col] = df_temporal_lee_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random2_centre'):\n",
    "        df_series_2yr_random2_temporal_lee[col] = df_temporal_lee_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random3_centre'):\n",
    "        df_series_2yr_random3_temporal_lee[col] = df_temporal_lee_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "for col in df_temporal_frost_random.columns:\n",
    "    if col.endswith('_random1_centre'):\n",
    "        df_series_2yr_random1_temporal_frost[col] = df_temporal_frost_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random2_centre'):\n",
    "        df_series_2yr_random2_temporal_frost[col] = df_temporal_frost_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random3_centre'):\n",
    "        df_series_2yr_random3_temporal_frost[col] = df_temporal_frost_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "for col in df_quegan_random.columns:\n",
    "    if col.endswith('_random1_centre'):\n",
    "        df_series_2yr_random1_quegan[col] = df_quegan_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random2_centre'):\n",
    "        df_series_2yr_random2_quegan[col] = df_quegan_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random3_centre'):\n",
    "        df_series_2yr_random3_quegan[col] = df_quegan_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "for col in df_quegan_frost_random.columns:\n",
    "    if col.endswith('_random1_centre'):\n",
    "        df_series_2yr_random1_quegan_frost[col] = df_quegan_frost_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random2_centre'):\n",
    "        df_series_2yr_random2_quegan_frost[col] = df_quegan_frost_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random3_centre'):\n",
    "        df_series_2yr_random3_quegan_frost[col] = df_quegan_frost_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "for col in df_quegan3d_random.columns:\n",
    "    if col.endswith('_random1_centre'):\n",
    "        df_series_2yr_random1_quegan3d[col] = df_quegan3d_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random2_centre'):\n",
    "        df_series_2yr_random2_quegan3d[col] = df_quegan3d_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "    elif col.endswith('_random3_centre'):\n",
    "        df_series_2yr_random3_quegan3d[col] = df_quegan3d_random[col]\n",
    "        warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alltime_dfs = [df_series_alltime_gaps, [df_series_alltime_random1, df_series_alltime_random2, df_series_alltime_random3]]\n",
    "_2yr_dfs = [df_series_2yr_gaps, [df_series_2yr_random1,df_series_2yr_random2,df_series_2yr_random3]]\n",
    "\n",
    "_2yr_dfs_frost = [df_frost_gaps, [df_series_2yr_random1_frost,df_series_2yr_random2_frost,df_series_2yr_random3_frost]]\n",
    "_2yr_dfs_lee = [df_lee_gaps, [df_series_2yr_random1_lee,df_series_2yr_random2_lee,df_series_2yr_random3_lee]]\n",
    "_2yr_dfs_temporal = [df_temporal_gaps, [df_series_2yr_random1_temporal,df_series_2yr_random2_temporal,df_series_2yr_random3_temporal]]\n",
    "_2yr_dfs_temporal_lee = [df_temporal_lee_gaps, [df_series_2yr_random1_temporal_lee,df_series_2yr_random2_temporal_lee,df_series_2yr_random3_temporal_lee]]\n",
    "_2yr_dfs_temporal_frost = [df_temporal_frost_gaps, [df_series_2yr_random1_temporal_frost,df_series_2yr_random2_temporal_frost,df_series_2yr_random3_temporal_frost]]\n",
    "_2yr_dfs_quegan = [df_quegan_gaps, [df_series_2yr_random1_quegan,df_series_2yr_random2_quegan,df_series_2yr_random3_quegan]]\n",
    "_2yr_dfs_temporal_quegan = [df_quegan_frost_gaps, [df_series_2yr_random1_quegan_frost,df_series_2yr_random2_quegan_frost,df_series_2yr_random3_quegan_frost]]\n",
    "_2yr_dfs_quegan3d = [df_quegan3d_gaps, [df_series_2yr_random1_quegan3d,df_series_2yr_random2_quegan3d,df_series_2yr_random3_quegan3d]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_variances(dfs):\n",
    "    variances = []\n",
    "    for df in dfs:\n",
    "        _, variance = mean_var_axis0(df)\n",
    "        variances.append(variance)\n",
    "    return variances\n",
    "\n",
    "def plot_boxplots(alltime_dfs, _2yr_dfs):\n",
    "    # Extract variances for each set of dataframes\n",
    "    variances_alltime = extract_variances([alltime_dfs[0]] + alltime_dfs[1])\n",
    "    variances_2yr = extract_variances([_2yr_dfs[0]] + _2yr_dfs[1])\n",
    "    \n",
    "    # Combine all variances for plotting\n",
    "    data_alltime = variances_alltime\n",
    "    data_2yr = variances_2yr\n",
    "\n",
    "    # Labels for the boxplots\n",
    "    labels_alltime = ['Gap', 'Random 1', 'Random 2', 'Random 3']\n",
    "    labels_2yr = ['Gap', 'Random 1', 'Random 2', 'Random 3']\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(6, 3), sharey=True)\n",
    "\n",
    "    # Boxplot properties\n",
    "    boxprops = dict(facecolor='blue', color='black')\n",
    "    medianprops = dict(color='red', linewidth=2)\n",
    "    whiskerprops = dict(color='black', linewidth=2)\n",
    "    capprops = dict(color='black', linewidth=2)\n",
    "    flierprops = dict(marker='o', color='blue', alpha=0.5)\n",
    "    \n",
    "    # Boxplot for Alltime dataframes\n",
    "    bp_alltime = axs[0].boxplot(data_alltime, patch_artist=True, boxprops=boxprops, medianprops=medianprops, whiskerprops=whiskerprops, capprops=capprops, flierprops=flierprops, showmeans=True)\n",
    "    axs[0].set_xticklabels(labels_alltime, rotation=45)\n",
    "    axs[0].set_ylabel('Variance')\n",
    "    axs[0].set_title('(a)')\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # Boxplot for 2-Year dataframes\n",
    "    bp_2yr = axs[1].boxplot(data_2yr, patch_artist=True, boxprops=boxprops, medianprops=medianprops, whiskerprops=whiskerprops, capprops=capprops, flierprops=flierprops, showmeans=True)\n",
    "    axs[1].set_xticklabels(labels_2yr, rotation=45)\n",
    "    axs[1].set_title('(b)')\n",
    "    axs[1].grid(True)\n",
    "\n",
    "    # Customize the mean points\n",
    "    for mean in bp_alltime['means']:\n",
    "        mean.set(marker='o', color='red', markersize=7)\n",
    "    for mean in bp_2yr['means']:\n",
    "        mean.set(marker='o', color='red', markersize=7)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "#plot_boxplots(alltime_dfs, _2yr_dfs)\n",
    "#plt.savefig('boxplot_variance.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_side_by_side(dfs1, dfs2, dfs3, labels1, labels2, labels3):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(6, 3), sharex=True, sharey=True)\n",
    "\n",
    "    # Initialize lists to hold all the scatter points for x and y\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "\n",
    "    def plot_scatter(ax, gap_df, random_dfs, color_gap, color_random):\n",
    "        (ensemble_mean, ensemble_var), (aggregate_mean, aggregate_var) = aggregate_and_ensemble_vals(gap_df)\n",
    "        all_x.extend([ensemble_mean, aggregate_mean])\n",
    "        all_y.extend([ensemble_var, aggregate_var])\n",
    "        ax.scatter(ensemble_mean, ensemble_var, color=color_gap, marker='x', s=100, label='Gap - Ensemble')\n",
    "        ax.scatter(aggregate_mean, aggregate_var, color=color_gap, marker='x', s=50, label='Gap - Aggregate')\n",
    "        for df in random_dfs:\n",
    "            (ensemble_mean, ensemble_var), (aggregate_mean, aggregate_var) = aggregate_and_ensemble_vals(df)\n",
    "            all_x.extend([ensemble_mean, aggregate_mean])\n",
    "            all_y.extend([ensemble_var, aggregate_var])\n",
    "            ax.scatter(ensemble_mean, ensemble_var, color=color_random, marker='o', s=100, label='Random - Ensemble', alpha=0.5)\n",
    "            ax.scatter(aggregate_mean, aggregate_var, color=color_random, marker='o', s=25, label='Random - Aggregate', alpha=0.5)\n",
    "\n",
    "    # Plot for the first set of dataframes (Alltime)\n",
    "    gap_df1, random_dfs1 = dfs1\n",
    "    print(type(gap_df1),type(random_dfs1))\n",
    "    plot_scatter(axs[0], gap_df1, random_dfs1, 'red', 'blue')\n",
    "    axs[0].set_title(labels1)\n",
    "    axs[0].set_xlabel('Mean')\n",
    "    axs[0].set_ylabel('Variance')\n",
    "    axs[0].grid()\n",
    "\n",
    "    # Plot for the second set of dataframes (2-Year)\n",
    "    gap_df2, random_dfs2 = dfs2\n",
    "    plot_scatter(axs[1], gap_df2, random_dfs2, 'red', 'blue')\n",
    "    axs[1].set_title(labels2)\n",
    "    axs[1].set_xlabel('Mean')\n",
    "    axs[1].grid()\n",
    "\n",
    "    # Plot for the third set of dataframes (Third set)\n",
    "    gap_df3, random_dfs3 = dfs3\n",
    "    plot_scatter(axs[2], gap_df3, random_dfs3, 'red', 'blue')\n",
    "    axs[2].set_title(labels3)\n",
    "    axs[2].set_xlabel('Mean')\n",
    "    axs[2].grid()\n",
    "\n",
    "    # Compute the min and max for x and y with a 0.2 border\n",
    "    x_min, x_max = min(all_x) - 0.3, max(all_x) + 0.3\n",
    "    y_min, y_max = min(all_y) - 0.3, max(all_y) + 0.3\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    # Custom legend\n",
    "    handles = [\n",
    "        plt.Line2D([0], [0], marker='x', color='w', label='Gap - Ensemble', markerfacecolor='none', markeredgecolor='red', markersize=10),\n",
    "        plt.Line2D([0], [0], marker='x', color='w', label='Gap - Aggregate', markerfacecolor='none', markeredgecolor='red', markersize=5),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', label='Random - Ensemble', markerfacecolor='blue', markeredgecolor='blue', markersize=12, alpha=0.5),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', label='Random - Aggregate', markerfacecolor='blue', markeredgecolor='blue', markersize=5, alpha=0.5)\n",
    "    ]\n",
    "    #fig.legend(handles=handles, loc='upper center', ncol=1, bbox_to_anchor=(0.55, 1.28))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plotting the scatter plots\n",
    "#labels_alltime = 'No filter (a)'\n",
    "#labels_2yr = 'Frost Filter (b)'\n",
    "#labels_3rd = 'Temp Lee Filter (c)'\n",
    "#plot_side_by_side(_2yr_dfs, _2yr_dfs_frost, _2yr_dfs_quegan, labels_alltime, labels_2yr, labels_3rd)\n",
    "#plt.savefig('filter_3_comparison.pdf')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_side_by_side_6(dfs1, dfs2, dfs3, dfs4, dfs5, dfs6, labels1, labels2, labels3, labels4, labels5, labels6):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(6, 6), sharex=True, sharey=True)\n",
    "\n",
    "    # Initialize lists to hold all the scatter points for x and y\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "\n",
    "    def plot_scatter(ax, gap_df, random_dfs, color_gap, color_random):\n",
    "        (ensemble_mean, ensemble_var), (aggregate_mean, aggregate_var) = aggregate_and_ensemble_vals(gap_df)\n",
    "        all_x.extend([ensemble_mean, aggregate_mean])\n",
    "        all_y.extend([ensemble_var, aggregate_var])\n",
    "        ax.scatter(ensemble_mean, ensemble_var, color=color_gap, marker='x', s=100, label='Gap - Ensemble')\n",
    "        ax.scatter(aggregate_mean, aggregate_var, color=color_gap, marker='x', s=50, label='Gap - Aggregate')\n",
    "        for df in random_dfs:\n",
    "            (ensemble_mean, ensemble_var), (aggregate_mean, aggregate_var) = aggregate_and_ensemble_vals(df)\n",
    "            all_x.extend([ensemble_mean, aggregate_mean])\n",
    "            all_y.extend([ensemble_var, aggregate_var])\n",
    "            ax.scatter(ensemble_mean, ensemble_var, color=color_random, marker='o', s=100, label='Random - Ensemble', alpha=0.5)\n",
    "            ax.scatter(aggregate_mean, aggregate_var, color=color_random, marker='o', s=25, label='Random - Aggregate', alpha=0.5)\n",
    "\n",
    "    # List of dataframes and labels\n",
    "    dataframe_list = [dfs1, dfs2, dfs3, dfs4, dfs5, dfs6]\n",
    "    labels_list = [labels1, labels2, labels3, labels4, labels5, labels6]\n",
    "\n",
    "    # Plot for each set of dataframes\n",
    "    for idx, (dfs, label) in enumerate(zip(dataframe_list, labels_list)):\n",
    "        row, col = divmod(idx, 3)\n",
    "        gap_df, random_dfs = dfs\n",
    "        plot_scatter(axs[row, col], gap_df, random_dfs, 'red', 'blue')\n",
    "        axs[row, col].set_title(label)\n",
    "        if row == 1:\n",
    "            axs[row, col].set_xlabel('Mean')\n",
    "        else:\n",
    "            axs[row, col].set_xlabel('')\n",
    "            axs[row, col].tick_params(axis='x', which='both', bottom=False)\n",
    "        if col == 0:\n",
    "            axs[row, col].set_ylabel('Variance')\n",
    "        else:\n",
    "            axs[row, col].set_ylabel('')\n",
    "            axs[row, col].tick_params(axis='y', which='both', left=False)\n",
    "        axs[row, col].grid()\n",
    "\n",
    "    # Compute the min and max for x and y with a 0.3 border\n",
    "    x_min, x_max = min(all_x) - 0.3, max(all_x) + 0.3\n",
    "    y_min, y_max = min(all_y) - 0.3, max(all_y) + 0.3\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    # Custom legend\n",
    "    handles = [\n",
    "        plt.Line2D([0], [0], marker='x', color='w', label='Gap - Ensemble', markerfacecolor='none', markeredgecolor='red', markersize=10),\n",
    "        plt.Line2D([0], [0], marker='x', color='w', label='Gap - Aggregate', markerfacecolor='none', markeredgecolor='red', markersize=5),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', label='Random - Ensemble', markerfacecolor='blue', markeredgecolor='blue', markersize=12, alpha=0.5),\n",
    "        plt.Line2D([0], [0], marker='o', color='w', label='Random - Aggregate', markerfacecolor='blue', markeredgecolor='blue', markersize=5, alpha=0.5)\n",
    "    ]\n",
    "    #fig.legend(handles=handles, loc='upper center', ncol=4, bbox_to_anchor=(0.5, 1.1))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your DataFrames are named as follows\n",
    "\n",
    "labels1 = 'No filter (a)'\n",
    "labels2 = 'Lee Filter (b)'\n",
    "labels3 = 'Frost Filter (c)'\n",
    "labels4 = 'M. Temp. Filter (d)'\n",
    "labels5 = 'Quegan Filter (e)'\n",
    "labels6 = 'Quegan 3D Filter (f)'\n",
    "\n",
    "\n",
    "plot_side_by_side_6(_2yr_dfs, _2yr_dfs_lee, _2yr_dfs_frost,_2yr_dfs_temporal,_2yr_dfs_quegan,_2yr_dfs_quegan3d, labels1,labels2,labels3,labels4, labels5, labels6)\n",
    "plt.savefig('filter_6_comparison_scatter.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVE AREA COLUMN FROM FEATURE DATAFRAMES SO WE CAN PLOT\n",
    "# only area is removed since its numerical, but areas are only present for gap datapoints, not random ones\n",
    "\n",
    "X_df_2yr_clean=X_df_2yr.drop(columns=['area'],axis=1)\n",
    "X_df_2yr_frost_clean=X_df_2yr_frost.drop(columns=['area'],axis=1)\n",
    "X_df_2yr_lee_clean = X_df_2yr_lee.drop(columns=['area'],axis=1)\n",
    "X_df_2yr_temporal_clean = X_df_2yr_temporal.drop(columns=['area'],axis=1)\n",
    "X_df_2yr_temporal_frost_clean = X_df_2yr_temporal_frost.drop(columns=['area'],axis=1)\n",
    "X_df_2yr_temporal_lee_clean=X_df_2yr_temporal_lee.drop(columns=['area'],axis=1)\n",
    "X_df_2yr_quegan_clean=X_df_2yr_quegan.drop(columns=['area'],axis=1)\n",
    "X_df_2yr_quegan_frost_clean=X_df_2yr_quegan_frost.drop(columns=['area'],axis=1)\n",
    "X_df_2yr_quegan3d_clean=X_df_2yr_quegan3d.drop(columns=['area'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dunn_index(X, labels):\n",
    "    \"\"\"\n",
    "    Dunn index as the ratio of the minimum inter-cluster distance to the maximum intra-cluster distance.\n",
    "    \n",
    "    - High Dunn Index:  well-separated and compact clusters (good).\n",
    "    - Low Dunn Index:  poorly separated and dispersed clusters (bad).\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Data points.\n",
    "        labels (np.ndarray): Cluster labels for each data point.\n",
    "\n",
    "    Returns:\n",
    "        float: The Dunn Index score.\n",
    "    \"\"\"\n",
    "    distances = squareform(pdist(X))\n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    intra_cluster_dists = []\n",
    "    inter_cluster_dists = []\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        cluster_points = X[labels == label]\n",
    "        if len(cluster_points) > 1:\n",
    "            intra_cluster_dists.append(np.mean(pdist(cluster_points)))\n",
    "        for other_label in unique_labels:\n",
    "            if label != other_label:\n",
    "                other_cluster_points = X[labels == other_label]\n",
    "                inter_cluster_dists.append(np.min([np.linalg.norm(a - b) for a in cluster_points for b in other_cluster_points]))\n",
    "                \n",
    "    return np.min(inter_cluster_dists) / np.max(intra_cluster_dists)\n",
    "\n",
    "def calculate_cluster_metrics(feature_matrix):\n",
    "    \"\"\"\n",
    "    Calculate clustering metrics for a given feature matrix.\n",
    "\n",
    "    This function computes the Silhouette Score, Davies-Bouldin Index, and Dunn Index to evaluate the quality of clustering.\n",
    "\n",
    "    - Silhouette Score:\n",
    "        - High Silhouette Score: Indicates well-separated clusters (good).\n",
    "        - Low Silhouette Score: Indicates overlapping clusters (bad).\n",
    "    - Davies-Bouldin Index:\n",
    "        - Low Davies-Bouldin Index: Indicates well-separated and compact clusters (good).\n",
    "        - High Davies-Bouldin Index: Indicates poorly separated and dispersed clusters (bad).\n",
    "    - Dunn Index:\n",
    "        - High Dunn Index: Indicates well-separated and compact clusters (good).\n",
    "        - Low Dunn Index: Indicates poorly separated and dispersed clusters (bad).\n",
    "    \n",
    "    Args:\n",
    "        feature_matrix (pd.DataFrame): DataFrame containing feature values and cluster labels.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Silhouette Score, Davies-Bouldin Index, Dunn Index.\n",
    "    \"\"\"\n",
    "    X = feature_matrix.drop(columns=['name', 'gap'])\n",
    "    labels = feature_matrix['gap']\n",
    "\n",
    "    silhouette = silhouette_score(X, labels)\n",
    "    davies_bouldin = davies_bouldin_score(X, labels)\n",
    "    dunn = dunn_index(X.values, labels.values)\n",
    "    \n",
    "    return silhouette, davies_bouldin, dunn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = [\n",
    "    \"X_df_2yr_clean\", \"X_df_2yr_frost_clean\", \"X_df_2yr_lee_clean\", \"X_df_2yr_temporal_clean\",\n",
    "    \"X_df_2yr_temporal_lee_clean\", \"X_df_2yr_temporal_frost_clean\", \"X_df_2yr_quegan_clean\",\"X_df_2yr_quegan3d_clean\"\n",
    "]\n",
    "\n",
    "# Dictionary to store the results\n",
    "results = {}\n",
    "\n",
    "# Loop through each dataframe, calculate metrics and store the results\n",
    "for df_name in dataframes_list:\n",
    "    feature_df = globals()[df_name]\n",
    "    silhouette, davies_bouldin, dunn = calculate_cluster_metrics(feature_df)\n",
    "    results[df_name] = [silhouette, davies_bouldin, dunn]\n",
    "\n",
    "# Create a dataframe from the results dictionary\n",
    "metrics_df = pd.DataFrame(results, index=[\"Silhouette Score\", \"Davies-Bouldin Index\", \"Dunn Index\"])\n",
    "\n",
    "metrics_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_list = [\n",
    "    \"X_df_2yr_clean\", \"X_df_2yr_frost_clean\", \"X_df_2yr_lee_clean\", \"X_df_2yr_temporal_clean\",\n",
    "    \"X_df_2yr_temporal_lee_clean\", \"X_df_2yr_temporal_frost_clean\", \"X_df_2yr_quegan_clean\",\"X_df_2yr_quegan3d_clean\"\n",
    "]\n",
    "\n",
    "correlation_results = {}\n",
    "\n",
    "# Loop through each DataFrame and calculate the absolute correlation with 'gap'\n",
    "for df_name in dataframes_list:\n",
    "    df = globals()[df_name]\n",
    "    numeric_features = df.drop(columns=['name'])\n",
    "    correlation_matrix = numeric_features.corr()\n",
    "    gap_correlations = correlation_matrix.loc['gap'].drop('gap').abs()  # Taking the absolute value\n",
    "    correlation_results[df_name] = gap_correlations\n",
    "\n",
    "correlation_df = pd.DataFrame(correlation_results).T\n",
    "\n",
    "# Define label mappings\n",
    "df_label_mapping = {\n",
    "    \"X_df_2yr_clean\": \"None\",\n",
    "    \"X_df_2yr_frost_clean\": \"Frost\",\n",
    "    \"X_df_2yr_lee_clean\": \"Lee\",\n",
    "    \"X_df_2yr_temporal_clean\": \"M. Temp.\",\n",
    "    \"X_df_2yr_temporal_lee_clean\": \"M. Temp. Lee\",\n",
    "    \"X_df_2yr_temporal_frost_clean\": \"M. Temp. Frost\",\n",
    "    \"X_df_2yr_quegan_clean\": \"Quegan Temporal\",\n",
    "    \"X_df_2yr_quegan3d_clean\":'3D Quegan',\n",
    "}\n",
    "\n",
    "# Replace DataFrame labels\n",
    "correlation_df.index = correlation_df.index.map(df_label_mapping)\n",
    "\n",
    "# Replace feature names if needed (example feature mapping)\n",
    "feature_label_mapping = {\n",
    "    \"mean\": \"Mean\",\n",
    "    \"variance\": \"Variance\",\n",
    "    \"skewness\": \"Skewness\",\n",
    "    \"max\": \"Maximum\",\n",
    "    \"min\": \"Minimum\",\n",
    "    \"range\": \"Range\"\n",
    "}\n",
    "\n",
    "correlation_df.columns = correlation_df.columns.map(feature_label_mapping)\n",
    "\n",
    "# Plot the correlation summary table with absolute values\n",
    "plt.figure(figsize=(6.1, 4))\n",
    "sns.heatmap(correlation_df, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, linewidths=0.5, vmin=0, vmax=0.27)\n",
    "plt.xlabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_table.pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_to_bottom_scatter(X_df1, X_df2, title1, title2):\n",
    "    # Create a figure with two subplots, one on top of the other\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 6), sharex=True,sharey=True)\n",
    "    \n",
    "    # Plot for the first dataframe\n",
    "    gap_points1 = X_df1[X_df1['gap'] == 1]\n",
    "    non_gap_points1 = X_df1[X_df1['gap'] == 0]\n",
    "    \n",
    "    ax1.scatter(non_gap_points1['mean'], non_gap_points1['variance'], color='blue', marker='o', label='Random',s=20)\n",
    "    ax1.scatter(gap_points1['mean'], gap_points1['variance'], color='red', marker='x', label='Gap',s=25)\n",
    "    ax1.set_ylabel('Variance')\n",
    "    ax1.legend(loc = 'upper left')\n",
    "    ax1.grid(True)\n",
    "    ax1.text(1.02, 0.5, '(a)', transform=ax1.transAxes, verticalalignment='center', fontweight='bold')\n",
    "\n",
    "\n",
    "    # Plot for the second dataframe\n",
    "    gap_points2 = X_df2[X_df2['gap'] == 1]\n",
    "    non_gap_points2 = X_df2[X_df2['gap'] == 0]\n",
    "\n",
    "    ax2.scatter(non_gap_points2['mean'], non_gap_points2['variance'], color='blue', marker='o', label='Random',s=20)\n",
    "    ax2.scatter(gap_points2['mean'], gap_points2['variance'], color='red', marker='x', label='Gap',s=25)\n",
    "    ax2.set_xlabel('Mean')\n",
    "    ax2.set_ylabel('Variance')\n",
    "    ax2.legend(loc = 'upper left')\n",
    "    ax2.grid(True)\n",
    "    ax2.text(1.02, 0.5, '(b)', transform=ax2.transAxes, verticalalignment='center', fontweight='bold')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Example usage\n",
    "#plot_top_to_bottom_scatter(X_df_2yr, X_df_2yr_frost, 'All Time Data', '2 Year Data')\n",
    "#plt.savefig('allscatter_double.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca_and_plot_combined(X_df1, title1, X_df2, title2, X_df3, title3):\n",
    "    # Define a helper function to perform PCA and return principal components and explained variance\n",
    "    def perform_pca(X_df):\n",
    "        features = X_df.drop(columns=['name', 'gap', 'area'], errors='ignore')\n",
    "        pca = PCA(n_components=min(len(features.columns), 6))  # Adjust n_components to the number of features or 6\n",
    "        principal_components = pca.fit_transform(features.fillna(0))  # Fill NaN with 0 for PCA\n",
    "        pca_df = pd.DataFrame(data=principal_components, columns=[f'Principal Component {i+1}' for i in range(pca.n_components_)])\n",
    "        pca_df['gap'] = X_df['gap']\n",
    "        explained_variance = pca.explained_variance_ratio_\n",
    "        return pca_df, explained_variance\n",
    "\n",
    "    # Perform PCA on the three datasets\n",
    "    pca_df1, explained_variance1 = perform_pca(X_df1)\n",
    "    pca_df2, explained_variance2 = perform_pca(X_df2)\n",
    "    pca_df3, explained_variance3 = perform_pca(X_df3)\n",
    "\n",
    "    # Plot the PCA results side by side\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(6.1, 6.1))\n",
    "\n",
    "    # Plot for the first dataset\n",
    "    sns.scatterplot(x='Principal Component 1', y='Principal Component 2', hue='gap', data=pca_df1[pca_df1['gap'] == 0],\n",
    "                    palette={0: 'blue'}, style='gap', markers={0: 'o'}, ax=axes[0, 0], zorder=1)\n",
    "    sns.scatterplot(x='Principal Component 1', y='Principal Component 2', hue='gap', data=pca_df1[pca_df1['gap'] == 1],\n",
    "                    palette={1: 'red'}, style='gap', markers={1: 'X'}, ax=axes[0, 0], zorder=2)\n",
    "    axes[0, 0].set_xlim(-6.5, 11)\n",
    "    axes[0, 0].set_ylim(-5, 8)\n",
    "    axes[0, 0].grid(True)\n",
    "    axes[0, 0].set_xlabel('')\n",
    "    axes[0, 0].set_xticklabels([])\n",
    "    axes[0, 0].tick_params(axis='x', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    axes[0,0].set_title(f'(a) {title1}')\n",
    "    axes[0, 0].get_legend().remove()\n",
    "\n",
    "    # Plot for the second dataset\n",
    "    sns.scatterplot(x='Principal Component 1', y='Principal Component 2', hue='gap', data=pca_df2[pca_df2['gap'] == 0],\n",
    "                    palette={0: 'blue'}, style='gap', markers={0: 'o'}, ax=axes[0, 1], zorder=1)\n",
    "    sns.scatterplot(x='Principal Component 1', y='Principal Component 2', hue='gap', data=pca_df2[pca_df2['gap'] == 1],\n",
    "                    palette={1: 'red'}, style='gap', markers={1: 'X'}, ax=axes[0, 1], zorder=2)\n",
    "    axes[0, 1].set_xlim(-6.5, 11)\n",
    "    axes[0, 1].set_ylim(-5, 8)\n",
    "    axes[0, 1].set_ylabel('')\n",
    "    axes[0, 1].set_xlabel('')\n",
    "    axes[0, 1].set_xticklabels([])\n",
    "    axes[0, 1].set_yticklabels([])\n",
    "    axes[0, 1].grid(True)\n",
    "    axes[0, 1].tick_params(axis='x', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    axes[0, 1].tick_params(axis='y', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    axes[0,1].set_title(f'(b) {title2}')\n",
    "    axes[0, 1].get_legend().remove()\n",
    "\n",
    "    # Plot for the third dataset in the bottom right position (axes[1, 1])\n",
    "    sns.scatterplot(x='Principal Component 1', y='Principal Component 2', hue='gap', data=pca_df3[pca_df3['gap'] == 0],\n",
    "                    palette={0: 'blue'}, style='gap', markers={0: 'o'}, ax=axes[1, 1], zorder=1)\n",
    "    sns.scatterplot(x='Principal Component 1', y='Principal Component 2', hue='gap', data=pca_df3[pca_df3['gap'] == 1],\n",
    "                    palette={1: 'red'}, style='gap', markers={1: 'X'}, ax=axes[1, 1], zorder=2)\n",
    "    axes[1, 1].set_xlim(-6.5, 11)\n",
    "    axes[1, 1].set_ylim(-5, 8)\n",
    "    axes[1, 1].grid(True)\n",
    "    axes[1,1].set_ylabel('')\n",
    "    axes[1, 1].set_yticklabels([])\n",
    "    axes[1, 1].tick_params(axis='y', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "    axes[1,1].set_title(f'(c) {title3}')\n",
    "    red_marker = mlines.Line2D([], [], color='red', marker='x', linestyle='None', markersize=7, label='Tree Gap')\n",
    "    blue_marker = mlines.Line2D([], [], color='blue', marker='o', linestyle='None', markersize=7, label='Random Point')\n",
    "\n",
    "# Add the custom legend to axes[1, 1]\n",
    "    axes[1, 1].legend(handles=[red_marker, blue_marker], loc='best', title='')\n",
    "\n",
    "    # Plot the scree plot with explained variance in the bottom left position (axes[1, 0])\n",
    "    ax_scree = fig.add_subplot(2, 2, 3)  # Changing from (2, 2, 4) to (2, 2, 3)\n",
    "    axes[1, 0].set_xticks([])\n",
    "    axes[1, 0].set_yticks([])\n",
    "    components = range(1, len(explained_variance1) + 1)\n",
    "    ax_scree.plot(components, explained_variance1, marker='o', markersize=4, label=title1, color='#3a86ff')\n",
    "    ax_scree.plot(components, explained_variance2, marker='x', markersize=4, label=title2, color='#ff006e')\n",
    "    ax_scree.plot(components, explained_variance3, marker='s', markersize=4, label=title3, color='#ffbe0b')\n",
    "    ax_scree.set_xlabel('Principal Components')\n",
    "    ax_scree.set_ylabel('Explained Variance')\n",
    "    ax_scree.legend(title='Filter')\n",
    "    ax_scree.grid(True)\n",
    "    ax_scree.set_title('(d) Scree Plot')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Example usage\n",
    "#perform_pca_and_plot_combined(X_df_2yr_clean, 'None', X_df_2yr_temporal_lee_clean, 'Temporal Lee', X_df_2yr_quegan_clean, 'Quegan Temporal')\n",
    "#plt.savefig('pca_filter_plots.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_variance(time_series):\n",
    "    \"\"\"\n",
    "    Calculate the mean and variance of a given time series.\n",
    "\n",
    "    Parameters:\n",
    "    time_series (list): The input time series data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the mean and variance of the time series.\n",
    "    \"\"\"\n",
    "    n = len(time_series)\n",
    "    mean = sum(time_series) / n\n",
    "    variance = sum((x - mean) ** 2 for x in time_series) / (n - 1)\n",
    "    \n",
    "    return mean, variance\n",
    "\n",
    "# Example usage:\n",
    "time_series_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "mean, variance = calculate_mean_variance(time_series_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_prime=X_df.sort_values(by='area',ascending=False).iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "below_100 = X_df[X_df['area'] < 39].shape[0]\n",
    "\n",
    "# Count the number of entries with area 100 and above\n",
    "above_100 = X_df[X_df['area'] >= 100].shape[0]\n",
    "\n",
    "below_100, above_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(X_df_prime['area'], bins=20, edgecolor='black')  # You can adjust the number of bins as needed\n",
    "plt.title('Histogram of Area')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_dataframes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataframes_list = [X_df_2yr_clean, X_df_2yr_frost_clean,X_df_2yr_lee_clean,X_df_2yr_temporal_clean,X_df_2yr_temporal_frost_clean,X_df_2yr_temporal_lee_clean,X_df_2yr_quegan_clean,X_df_2yr_quegan_frost_clean,X_df_2yr_quegan3d_clean]\n",
    "\n",
    "timeseries_dataframes_list = [[df_series_2yr_gaps,df_series_2yr_random],[df_frost_gaps, df_frost_random],[df_lee_gaps, df_lee_random],[df_temporal_gaps,df_temporal_random],[df_temporal_lee_gaps, df_temporal_lee_random],[df_temporal_frost_gaps,df_temporal_frost_random],[df_quegan_gaps,df_quegan_random],[df_quegan_frost_gaps, df_quegan_frost_random],[df_quegan3d_gaps, df_quegan3d_random]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_random_forest_and_plot_roc(df):\n",
    "    # Separate features and target variable\n",
    "    X = df.drop(columns=['name', 'gap'])\n",
    "    y = df['gap']\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train the random forest model\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc_score = roc_auc_score(y_test, y_probs)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def train_random_forest_and_plot_roc(df, ax, title):\n",
    "    # Separate features and target variable\n",
    "    X = df.drop(columns=['name', 'gap'])\n",
    "    y = df['gap']\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "    \n",
    "    # Train the random forest model\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc_score = roc_auc_score(y_test, y_probs)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "def plot_all_rocs(dataframes_list):\n",
    "    # Create a 3x3 grid of subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(7, 7))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop through the dataframes and plot ROC curves\n",
    "    for idx, df in enumerate(dataframes_list):\n",
    "        df_name = f'DataFrame {idx+1}'\n",
    "        train_random_forest_and_plot_roc(df, axes[idx], df_name)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_rocs(feature_dataframes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "def train_xgboost_and_plot_roc(df, ax, title):\n",
    "    # Separate features and target variable\n",
    "    X = df.drop(columns=['name', 'gap'])\n",
    "    y = df['gap']\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=40)\n",
    "    \n",
    "    # Train the XGBoost model\n",
    "    model = xgb.XGBClassifier(random_state=40, eval_metric='logloss', verbosity=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_probs = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc_score = roc_auc_score(y_test, y_probs)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "def plot_all_rocs(dataframes_list):\n",
    "    # Create a 3x3 grid of subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop through the dataframes and plot ROC curves\n",
    "    for idx, df in enumerate(dataframes_list):\n",
    "        df_name = f'DataFrame {idx+1}'\n",
    "        train_xgboost_and_plot_roc(df, axes[idx], df_name)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with your list of dataframes\n",
    "plot_all_rocs(feature_dataframes_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_xgboost_and_plot_roc(df, ax, title):\n",
    "    # Separate features and target variable\n",
    "    X = df.drop(columns=['name', 'gap'])\n",
    "    y = df['gap']\n",
    "    \n",
    "    # Handle class imbalance using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.15, random_state=42)\n",
    "    \n",
    "    # Hyperparameter tuning using GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6, 10],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    model = xgb.XGBClassifier(random_state=42, eval_metric='logloss', use_label_encoder=False)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model from GridSearchCV\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_probs = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "    \n",
    "    # Calculate AUC score\n",
    "    auc_score = roc_auc_score(y_test, y_probs)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "def plot_all_rocs(dataframes_list):\n",
    "    # Create a 3x3 grid of subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop through the dataframes and plot ROC curves\n",
    "    for idx, df in enumerate(dataframes_list):\n",
    "        df_name = f'DataFrame {idx+1}'\n",
    "        train_xgboost_and_plot_roc(df, axes[idx], df_name)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with your list of dataframes\n",
    "plot_all_rocs(feature_dataframes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "def train_xgboost_and_plot_roc(df, ax, title, random_states):\n",
    "    X = df.drop(columns=['name', 'gap'])\n",
    "    y = df['gap']\n",
    "    \n",
    "    fprs, tprs, aucs = [], [], []\n",
    "    \n",
    "    for state in random_states:\n",
    "        # Handle class imbalance using SMOTE\n",
    "        smote = SMOTE(random_state=state)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.15, random_state=state)\n",
    "        \n",
    "        # Hyperparameter tuning using GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 6, 10],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "        }\n",
    "        model = xgb.XGBClassifier(random_state=state, eval_metric='logloss', use_label_encoder=False)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get the best model from GridSearchCV\n",
    "        best_model = grid_search.best_estimator_\n",
    "        \n",
    "        # Predict probabilities\n",
    "        y_probs = best_model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "        \n",
    "        # Calculate AUC score\n",
    "        auc_score = roc_auc_score(y_test, y_probs)\n",
    "        \n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "        aucs.append(auc_score)\n",
    "    \n",
    "    # Calculate mean ROC curve\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    mean_tpr = np.mean([np.interp(mean_fpr, fprs[i], tprs[i]) for i in range(len(random_states))], axis=0)\n",
    "    mean_auc = np.mean(aucs)\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    ax.plot(mean_fpr, mean_tpr, label=f'ROC curve (Mean AUC = {mean_auc:.2f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "def plot_all_rocs(dataframes_list, random_states):\n",
    "    # Create a 3x3 grid of subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop through the dataframes and plot ROC curves\n",
    "    for idx, df in enumerate(dataframes_list):\n",
    "        df_name = f'DataFrame {idx+1}'\n",
    "        train_xgboost_and_plot_roc(df, axes[idx], df_name, random_states)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define a list of random states for averaging\n",
    "random_states = [42, 52, 62, 72, 82]\n",
    "\n",
    "# Example usage with your list of dataframes\n",
    "plot_all_rocs(feature_dataframes_list, random_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "def train_xgboost_and_collect_best_params(df, random_states):\n",
    "    X = df.drop(columns=['name', 'gap'])\n",
    "    y = df['gap']\n",
    "    \n",
    "    best_params = []\n",
    "    \n",
    "    for state in random_states:\n",
    "        # Handle class imbalance using SMOTE\n",
    "        smote = SMOTE(random_state=state)\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.15, random_state=state)\n",
    "        \n",
    "        # Hyperparameter tuning using GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [3, 6, 10],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.6, 0.8, 1.0],\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "        }\n",
    "        model = xgb.XGBClassifier(random_state=state, eval_metric='logloss', use_label_encoder=False)\n",
    "        grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Get the best parameters from GridSearchCV\n",
    "        best_params.append(grid_search.best_params_)\n",
    "        \n",
    "    return best_params\n",
    "\n",
    "def evaluate_best_model(df, best_params, random_state):\n",
    "    X = df.drop(columns=['name', 'gap'])\n",
    "    y = df['gap']\n",
    "    \n",
    "    # Handle class imbalance using SMOTE\n",
    "    smote = SMOTE(random_state=random_state)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.15, random_state=random_state)\n",
    "    \n",
    "    # Create a model with the best parameters\n",
    "    model = xgb.XGBClassifier(**best_params, random_state=random_state, eval_metric='logloss', use_label_encoder=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate precision and accuracy\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return precision, accuracy\n",
    "\n",
    "def plot_all_rocs(dataframes_list, random_states):\n",
    "    # Create a 3x3 grid of subplots\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Dictionary to store best parameters for each dataframe and each seed\n",
    "    all_best_params = {}\n",
    "    all_metrics = {}\n",
    "    \n",
    "    # Loop through the dataframes and plot ROC curves\n",
    "    for idx, df in enumerate(dataframes_list):\n",
    "        df_name = f'DataFrame {idx+1}'\n",
    "        best_params = train_xgboost_and_collect_best_params(df, random_states)\n",
    "        all_best_params[df_name] = best_params\n",
    "        \n",
    "        # Evaluate model with best parameters\n",
    "        metrics = []\n",
    "        for state, params in zip(random_states, best_params):\n",
    "            precision, accuracy = evaluate_best_model(df, params, state)\n",
    "            metrics.append({'random_state': state, 'precision': precision, 'accuracy': accuracy})\n",
    "        all_metrics[df_name] = metrics\n",
    "        \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return all_best_params, all_metrics\n",
    "\n",
    "# Define a list of random states for averaging\n",
    "random_states = [42, 52, 62, 72, 82]\n",
    "\n",
    "# Example usage with your list of dataframes\n",
    "all_best_params, all_metrics = plot_all_rocs(feature_dataframes_list, random_states)\n",
    "\n",
    "# Print the best parameters and metrics for each dataframe and each random state\n",
    "for df_name, params in all_best_params.items():\n",
    "    print(f\"{df_name}:\")\n",
    "    for state, param in zip(random_states, params):\n",
    "        print(f\"  Random State {state}: {param}\")\n",
    "\n",
    "for df_name, metrics in all_metrics.items():\n",
    "    print(f\"{df_name}:\")\n",
    "    for metric in metrics:\n",
    "        print(f\"  Random State {metric['random_state']}: Precision = {metric['precision']:.2f}, Accuracy = {metric['accuracy']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
